{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haystack Hotspots Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import subprocess as sb\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.use('Agg')\n",
    "import pylab as pl\n",
    "import argparse\n",
    "import logging\n",
    "import xml.etree.cElementTree as ET\n",
    "# commmon functions\n",
    "from haystack_common import determine_path, which, check_file\n",
    "\n",
    "from pybedtools import BedTool\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create logging file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAYSTACK_VERSION = \"0.4.0\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(levelname)-5s @ %(asctime)s:\\n\\t %(message)s \\n',\n",
    "                    datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                    stream=sys.stderr,\n",
    "                    filemode=\"w\"\n",
    "                    )\n",
    "error = logging.critical\n",
    "warn = logging.warning\n",
    "debug = logging.debug\n",
    "info = logging.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantile_normalization(A):\n",
    "    AA = np.zeros_like(A)\n",
    "    I = np.argsort(A, axis=0)\n",
    "    AA[I, np.arange(A.shape[1])] = np.mean(A[I, np.arange(A.shape[1])], axis=1)[:, np.newaxis]\n",
    "\n",
    "    return AA\n",
    "\n",
    "\n",
    "def smooth(x, window_len=200):\n",
    "    s = np.r_[x[window_len - 1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.hanning(window_len)\n",
    "    y = np.convolve(w / w.sum(), s, mode='valid')\n",
    "    return y[int(window_len / 2):-int(window_len / 2) + 1]\n",
    "\n",
    "\n",
    "# write the IGV session file\n",
    "def rem_base_path(path, base_path):\n",
    "    return path.replace(os.path.join(base_path, ''), '')\n",
    "\n",
    "\n",
    "def find_th_rpm(df_chip, th_rpm):\n",
    "    return np.min(df_chip.apply(lambda x: np.percentile(x, th_rpm)))\n",
    "\n",
    "\n",
    "def log2_transform(x):\n",
    "    return np.log2(x + 1)\n",
    "\n",
    "\n",
    "def angle_transform(x):\n",
    "    return np.arcsin(np.sqrt(x) / 1000000.0)\n",
    "\n",
    "\n",
    "# def normalize_count(feature, scaling_factor):\n",
    "#     feature.name = str(int(feature.name) * scaling_factor)\n",
    "#     return feature\n",
    "\n",
    "def get_scaling_factor(bam_filename):\n",
    "    from pysam import AlignmentFile\n",
    "\n",
    "    infile = AlignmentFile(bam_filename, \"rb\")\n",
    "    numreads = infile.count(until_eof=True)\n",
    "    scaling_factor = (1.0 / float(numreads)) * 1000000\n",
    "    return scaling_factor\n",
    "\n",
    "\n",
    "def check_required_packages():\n",
    "    if which('samtools') is None:\n",
    "        error(\n",
    "            'Haystack requires samtools. Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('bedtools') is None:\n",
    "        error('Haystack requires bedtools. Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('bedGraphToBigWig') is None:\n",
    "        info(\n",
    "            'To generate the bigwig files Haystack requires bedGraphToBigWig. Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('sambamba') is None:\n",
    "        info(\n",
    "            'Haystack requires sambamba. Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('bigWigAverageOverBed') is None:\n",
    "        info(\n",
    "            'Haystack requires bigWigAverageOverBed. Please install using bioconda')\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create parse argument function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # mandatory\n",
    "    parser = argparse.ArgumentParser(description='HAYSTACK Parameters')\n",
    "    parser.add_argument('samples_filename', type=str,\n",
    "                        help='A tab delimited file with in each row (1) a sample name, (2) the path to the corresponding bam or bigwig filename')\n",
    "    parser.add_argument('genome_name', type=str, help='Genome assembly to use from UCSC (for example hg19, mm9, etc.)')\n",
    "\n",
    "    # optional\n",
    "    parser.add_argument('--bin_size', type=int, help='bin size to use(default: 500bp)', default=500)\n",
    "    parser.add_argument('--disable_quantile_normalization', help='Disable quantile normalization (default: False)',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--th_rpm', type=float,\n",
    "                        help='Percentile on the signal intensity to consider for the hotspots (default: 99)',\n",
    "                        default=99)\n",
    "    parser.add_argument('--transformation', type=str,\n",
    "                        help='Variance stabilizing transformation among: none, log2, angle (default: angle)',\n",
    "                        default='angle', choices=['angle', 'log2', 'none'])\n",
    "    parser.add_argument('--recompute_all', help='Ignore any file previously precalculated', action='store_true')\n",
    "    parser.add_argument('--z_score_high', type=float, help='z-score value to select the specific regions(default: 1.5)',\n",
    "                        default=1.5)\n",
    "    parser.add_argument('--z_score_low', type=float,\n",
    "                        help='z-score value to select the not specific regions(default: 0.25)', default=0.25)\n",
    "    parser.add_argument('--name', help='Define a custom output filename for the report', default='')\n",
    "    parser.add_argument('--output_directory', type=str, help='Output directory (default: current directory)',\n",
    "                        default='')\n",
    "    parser.add_argument('--blacklist', help='Exclude blacklisted regions.', action='store_true')\n",
    "    parser.add_argument('--chrom_exclude', help='Exclude chromosomes. For example (_|chrM|chrX|chrY).',\n",
    "                        default='chrX|chrY')\n",
    "    parser.add_argument('--read_ext', type=int, help='Read extension', default='200')\n",
    "    parser.add_argument('--max_regions_percentage', type=float,\n",
    "                        help='Upper bound on the %% of the regions selected  (default: 0.1, 0.0=0%% 1.0=100%%)',\n",
    "                        default=0.1)\n",
    "    parser.add_argument('--depleted',\n",
    "                        help='Look for cell type specific regions with depletion of signal instead of enrichment',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--input_is_bigwig',\n",
    "                        help='Use the bigwig format instead of the bam format for the input. Note: The files must have extension .bw',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--n_processes', type=int, help='Specify the number of processes to use. The default is #cores available.', default=multiprocessing.cpu_count())\n",
    "    parser.add_argument('--version', help='Print version and exit.', action='version',\n",
    "                        version='Version %s' % HAYSTACK_VERSION)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_filepaths(samples_filename):\n",
    "    # check folder or sample filename\n",
    "    if os.path.isfile(samples_filename):\n",
    "        data_filenames = []\n",
    "        sample_names = []\n",
    "        with open(samples_filename) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "\n",
    "                if line.startswith('#'):  # skip optional header line or empty lines\n",
    "                    info('Skipping header/comment line:%s' % line)\n",
    "                    continue\n",
    "\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                n_fields = len(fields)\n",
    "\n",
    "                if n_fields == 2:\n",
    "                    sample_names.append(fields[0])\n",
    "                    data_filenames.append(fields[1])\n",
    "                else:\n",
    "                    error('The samples file format is wrong!')\n",
    "                    sys.exit(1)\n",
    "\n",
    "    dir_path = os.path.dirname(os.path.realpath(samples_filename))\n",
    "    data_filenames = [os.path.join(dir_path, filename) for filename in data_filenames]\n",
    "    # check all the files before starting\n",
    "    info('Checking samples files location...')\n",
    "    for data_filename in data_filenames:\n",
    "        check_file(data_filename)\n",
    "\n",
    "    return sample_names, data_filenames\n",
    "\n",
    "def initialize_genome(genome_name):\n",
    "    from bioutilities import Genome_2bit\n",
    "\n",
    "    info('Initializing Genome:%s' % genome_name)\n",
    "\n",
    "    genome_directory = determine_path('genomes')\n",
    "\n",
    "    genome_2bit = os.path.join(genome_directory, genome_name + '.2bit')\n",
    "    chr_len_filename = os.path.join(genome_directory, \"%s_chr_lengths.txt\" % genome_name)\n",
    "\n",
    "    if os.path.exists(genome_2bit):\n",
    "        Genome_2bit(genome_2bit)\n",
    "    else:\n",
    "        info(\"\\nIt seems you don't have the required genome file.\")\n",
    "\n",
    "        download_genome(genome_name, genome_directory)\n",
    "        if os.path.exists(genome_2bit):\n",
    "            info('Genome correctly downloaded!')\n",
    "            Genome_2bit(genome_2bit)\n",
    "        else:\n",
    "            error('Sorry I cannot download the required file for you. Check your Internet connection.')\n",
    "            sys.exit(1)\n",
    "\n",
    "    check_file(chr_len_filename)\n",
    "\n",
    "    return chr_len_filename\n",
    "\n",
    "def create_tiled_genome(genome_name, output_directory, chr_len_filename, bin_size,\n",
    "                        chrom_exclude, blacklist):\n",
    "    from re import search\n",
    "\n",
    "    genome_directory = determine_path('genomes')\n",
    "\n",
    "    genome_sorted_bins_file = os.path.join(output_directory, '%s.%dbp.bins.sorted.bed'\n",
    "                                           % (os.path.basename(genome_name), bin_size))\n",
    "\n",
    "    if not os.path.exists(genome_sorted_bins_file) or recompute_all:\n",
    "\n",
    "        info('Creating bins of %dbp in %s' % (bin_size, genome_sorted_bins_file))\n",
    "\n",
    "        if chrom_exclude:\n",
    "            chr_len_filtered_filename = os.path.join(genome_directory,\n",
    "                                                     \"%s_chr_lengths_filtered.txt\" % genome_name)\n",
    "\n",
    "            with open(chr_len_filtered_filename, 'wb') as f:\n",
    "                f.writelines(line for line in open(chr_len_filename)\n",
    "                             if not search(chrom_exclude, line.split()[0]))\n",
    "        else:\n",
    "            chr_len_filtered_filename = chr_len_filename\n",
    "\n",
    "        tiled_genome = BedTool(). \\\n",
    "            window_maker(g=chr_len_filtered_filename,\n",
    "                         w=bin_size).sort()\n",
    "\n",
    "        if blacklist:\n",
    "            \n",
    "            info('Filtering blacklisted regions')\n",
    "\n",
    "            blacklist_filepath = os.path.join(genome_directory,\n",
    "                                              'blacklist.bed')\n",
    "            check_file(blacklist_filepath)\n",
    "\n",
    "            tiled_genome.intersect(blacklist_filepath,\n",
    "                                   wa=True,\n",
    "                                   v=True,\n",
    "                                   output=genome_sorted_bins_file)\n",
    "        else:\n",
    "            tiled_genome.saveas(genome_sorted_bins_file)\n",
    "            \n",
    "        info('Saved file. Filepath: %s' %  genome_sorted_bins_file)\n",
    "   \n",
    "    return genome_sorted_bins_file\n",
    "\n",
    "\n",
    "### if bigwig\n",
    "\n",
    "def copy_bigwigs(data_filenames, sample_names, tracks_directory):\n",
    "    bigwig_filenames = [os.path.join(tracks_directory,\n",
    "                                     '%s.bw' % sample_name)\n",
    "                        for sample_name in sample_names]\n",
    "\n",
    "    for data_filename, bigwig_filename in zip(data_filenames, bigwig_filenames):\n",
    "        shutil.copy2(data_filename, bigwig_filename)\n",
    "\n",
    "    return bigwig_filenames\n",
    "\n",
    "\n",
    "def to_binned_tracks_if_bigwigs(data_filenames, intermediate_directory, binned_sample_names, genome_sorted_bins_file):\n",
    "    binned_rpm_filenames = [os.path.join(intermediate_directory,\n",
    "                                         '%s.rpm' % binned_sample_name)\n",
    "                            for binned_sample_name in binned_sample_names]\n",
    "\n",
    "    for data_filename, binned_rpm_filename in zip(data_filenames, binned_rpm_filenames):\n",
    "\n",
    "        if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "            info('Processing:%s' % data_filename)\n",
    "\n",
    "            cmd = 'bigWigAverageOverBed %s %s  /dev/stdout | sort -s -n -k 1,1 | cut -f5 > %s' % (\n",
    "                data_filename, genome_sorted_bins_file, binned_rpm_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "    return binned_rpm_filenames\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "def to_filtered_deduped_bams(bam_filenames, output_directory, n_processes):\n",
    "    filtered_bam_directory = os.path.join(output_directory, 'FILTERED_BAMS')\n",
    "    if not os.path.exists(filtered_bam_directory):\n",
    "        os.makedirs(filtered_bam_directory)\n",
    "\n",
    "    bam_filtered_nodup_filenames = [os.path.join(\n",
    "        filtered_bam_directory,\n",
    "        '%s.filtered.nodup%s' % (os.path.splitext(os.path.basename(bam_filename))))\n",
    "        for bam_filename in bam_filenames]\n",
    "\n",
    "    for bam_filename, bam_filtered_nodup_filename in zip(bam_filenames, bam_filtered_nodup_filenames):\n",
    "\n",
    "        if not os.path.exists(bam_filtered_nodup_filename) or recompute_all:\n",
    "            info('Filtering and deduping: %s' % bam_filename)\n",
    "            bam_temp_filename = os.path.join(os.path.dirname(bam_filtered_nodup_filename),\n",
    "                                             '%s.temp%s' % os.path.splitext(\n",
    "                                                 os.path.basename(bam_filtered_nodup_filename)))\n",
    "            info('Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc')\n",
    "\n",
    "            # cmd = 'sambamba view -f bam -l 0 -t %d -F \"not (unmapped or mate_is_unmapped or failed_quality_control or duplicate or secondary_alignment) and mapping_quality >= 30\" \"%s\"  -o \"%s\"' % (\n",
    "            #     n_processes, bam_filename, bam_temp_filename)\n",
    "\n",
    "            cmd = 'sambamba view -f bam -l 0 -t %d -F \"not failed_quality_control\" \"%s\"  -o \"%s\"' % (n_processes,\n",
    "                                                                                                     bam_filename,\n",
    "                                                                                                     bam_temp_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "            info('Removing  optical duplicates')\n",
    "            cmd = 'sambamba markdup  -l 5 -t %d --hash-table-size=17592186044416 --overflow-list-size=20000000 --io-buffer-size=256 \"%s\" \"%s\" ' % (\n",
    "                n_processes,\n",
    "                bam_temp_filename,\n",
    "                bam_filtered_nodup_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "            try:\n",
    "                os.remove(bam_temp_filename)\n",
    "                os.remove(bam_temp_filename + '.bai')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return bam_filtered_nodup_filenames\n",
    "\n",
    "def to_normalized_extended_reads_tracks(bam_filenames, sample_names, tracks_directory, chr_len_filename, read_ext):\n",
    "    bedgraph_filenames = [os.path.join(tracks_directory, '%s.bedgraph' % sample_name)\n",
    "                          for sample_name in sample_names]\n",
    "    bigwig_filenames = [filename.replace('.bedgraph', '.bw')\n",
    "                        for filename in bedgraph_filenames]\n",
    "\n",
    "    for bam_filename, bedgraph_filename, bigwig_filename in zip(bam_filenames,\n",
    "                                                                bedgraph_filenames,\n",
    "                                                                bigwig_filenames):\n",
    "\n",
    "        if not os.path.exists(bedgraph_filename) or recompute_all:\n",
    "            info('Normalizing and extending reads of %s' % bam_filename)\n",
    "\n",
    "            info('Computing Scaling Factor...')\n",
    "            scaling_factor = get_scaling_factor(bam_filename)\n",
    "            info('Scaling Factor: %e' % scaling_factor)\n",
    "\n",
    "            info('Converting bam to bed and extending read length...')\n",
    "            BedTool(bam_filename). \\\n",
    "                bam_to_bed(). \\\n",
    "                slop(r=read_ext,\n",
    "                     l=0,\n",
    "                     s=True,\n",
    "                     g=chr_len_filename). \\\n",
    "                genome_coverage(bg=True,\n",
    "                                scale=scaling_factor,\n",
    "                                g=chr_len_filename). \\\n",
    "                sort(). \\\n",
    "                saveas(bedgraph_filename)\n",
    "\n",
    "        if not os.path.exists(bigwig_filename) or recompute_all:\n",
    "            info('Converting BedGraph to BigWig')\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_filename,\n",
    "                                                       chr_len_filename,\n",
    "                                                       bigwig_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "            info('Computing coverage over bins...')\n",
    "            info('Normalizing counts by scaling factor...')\n",
    "\n",
    "    return bedgraph_filenames, bigwig_filenames\n",
    "\n",
    "\n",
    "def to_binned_tracks(bedgraph_filenames, binned_sample_names, tracks_directory, intermediate_directory,\n",
    "                     chr_len_filename, genome_sorted_bins_file):\n",
    "    bedgraph_binned_filenames = [os.path.join(tracks_directory,\n",
    "                                              '%s.bedgraph' % binned_sample_name)\n",
    "                                 for binned_sample_name in binned_sample_names]\n",
    "\n",
    "    binned_rpm_filenames = [os.path.join(intermediate_directory,\n",
    "                                         '%s.rpm' % binned_sample_name)\n",
    "                            for binned_sample_name in binned_sample_names]\n",
    "\n",
    "    bigwig_binned_filenames = [filename.replace('.bedgraph', '.bw')\n",
    "                               for filename in bedgraph_binned_filenames]\n",
    "\n",
    "    for bedgraph_filename, bedgraph_binned_filename, binned_rpm_filename, bigwig_binned_filename in zip(\n",
    "            bedgraph_filenames,\n",
    "            bedgraph_binned_filenames,\n",
    "            binned_rpm_filenames,\n",
    "            bigwig_binned_filenames):\n",
    "        if not os.path.exists(bedgraph_binned_filename) or recompute_all:\n",
    "            \n",
    "            info('Creating binned file: %s' % bedgraph_filename)\n",
    "\n",
    "            bedgraph = BedTool(genome_sorted_bins_file). \\\n",
    "                map(b=bedgraph_filename,\n",
    "                    c=4,\n",
    "                    o='mean',\n",
    "                    null=0.0). \\\n",
    "                saveas(bedgraph_binned_filename)\n",
    "\n",
    "            # bedgraph = BedTool(genome_sorted_bins_file). \\\n",
    "            #     intersect(bed_extended, c=True). \\\n",
    "            #     each(normalize_count, scaling_factor). \\\n",
    "            #     saveas(bedgraph_filename)\n",
    "\n",
    "            info('Binned Bedgraph saved...')\n",
    "\n",
    "        if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "            info('Making constant binned (%dbp) rpm values file' % bin_size)\n",
    "            bedgraph.to_dataframe()['name'].to_csv(binned_rpm_filename,\n",
    "                                                   index=False)\n",
    "\n",
    "        if not os.path.exists(bigwig_binned_filename) or recompute_all:\n",
    "            info('Converting BedGraph to BigWig: %s' % bigwig_binned_filename)\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_binned_filename,\n",
    "                                                       chr_len_filename,\n",
    "                                                       bigwig_binned_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "    return binned_rpm_filenames\n",
    "\n",
    "\n",
    "def load_binned_rpm_tracks(binned_sample_names, binned_rpm_filenames):\n",
    "    # load all the tracks\n",
    "    info('Loading the processed tracks')\n",
    "    df_chip = {}\n",
    "    for binned_sample_name, binned_rpm_filename in zip(binned_sample_names, binned_rpm_filenames):\n",
    "        df_chip[binned_sample_name] = pd.read_csv(binned_rpm_filename,\n",
    "                                                  squeeze=True,\n",
    "                                                  header=None)\n",
    "        info('Loading %s from file %s' % (binned_sample_name, binned_rpm_filename))\n",
    "\n",
    "    df_chip = pd.DataFrame(df_chip)\n",
    "\n",
    "    return df_chip\n",
    "\n",
    "\n",
    "def to_binned_normalized_tracks(df_chip, coordinates_bin, binned_sample_names, chr_len_filename, tracks_directory):\n",
    "    df_chip_normalized = pd.DataFrame(quantile_normalization(df_chip.values),\n",
    "                                      columns=df_chip.columns,\n",
    "                                      index=df_chip.index)\n",
    "\n",
    "    bedgraph_binned_normalized_filenames = [os.path.join(tracks_directory,\n",
    "                                                         '%s_quantile_normalized.bedgraph' % binned_sample_name)\n",
    "                                            for binned_sample_name in binned_sample_names]\n",
    "    bigwig_binned_normalized_filenames = [filename.replace('.bedgraph', '.bw')\n",
    "                                          for filename in bedgraph_binned_normalized_filenames]\n",
    "    # write quantile normalized tracks\n",
    "    for binned_sample_name, bedgraph_binned_normalized_filename, bigwig_binned_normalized_filename in zip(\n",
    "            binned_sample_names,\n",
    "            bedgraph_binned_normalized_filenames,\n",
    "            bigwig_binned_normalized_filenames):\n",
    "        if not os.path.exists(bedgraph_binned_normalized_filename) or recompute_all:\n",
    "            info('Writing binned track: %s' % bigwig_binned_normalized_filename)\n",
    "            joined_df = pd.DataFrame.join(coordinates_bin, df_chip_normalized[binned_sample_name])\n",
    "            joined_df.to_csv(bedgraph_binned_normalized_filename,\n",
    "                             sep='\\t',\n",
    "                             header=False,\n",
    "                             index=False)\n",
    "        if not os.path.exists(bigwig_binned_normalized_filename) or recompute_all:\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_binned_normalized_filename,\n",
    "                                                       chr_len_filename,\n",
    "                                                       bigwig_binned_normalized_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "    return df_chip_normalized, bigwig_binned_normalized_filenames\n",
    "\n",
    "\n",
    "def find_hpr_coordinates(df_chip, coordinates_bin, th_rpm, transformation, max_regions_percentage):\n",
    "    # th_rpm=args.th_rpm\n",
    "    # transformation=args.transformation\n",
    "    # max_regions_percentage=args.max_regions_percentage\n",
    "    # th_rpm=np.min(df_chip.apply(lambda x: np.percentile(x,th_rpm)))\n",
    "    th_rpm_est = find_th_rpm(df_chip, th_rpm)\n",
    "    info('Estimated th_rpm:%s' % th_rpm_est)\n",
    "\n",
    "    df_chip_not_empty = df_chip.loc[(df_chip > th_rpm_est).any(1), :]\n",
    "\n",
    "    if transformation == 'log2':\n",
    "        df_chip_not_empty = df_chip_not_empty.applymap(log2_transform)\n",
    "        info('Using log2 transformation')\n",
    "\n",
    "    elif transformation == 'angle':\n",
    "        df_chip_not_empty = df_chip_not_empty.applymap(angle_transform)\n",
    "        info('Using angle transformation')\n",
    "\n",
    "    else:\n",
    "        info('Using no transformation')\n",
    "\n",
    "    iod_values = df_chip_not_empty.var(1) / df_chip_not_empty.mean(1)\n",
    "\n",
    "    ####calculate the inflation point a la superenhancers\n",
    "    scores = iod_values\n",
    "    min_s = np.min(scores)\n",
    "    max_s = np.max(scores)\n",
    "\n",
    "    N_POINTS = len(scores)\n",
    "    x = np.linspace(0, 1, N_POINTS)\n",
    "    y = sorted((scores - min_s) / (max_s - min_s))\n",
    "    m = smooth((np.diff(y) / np.diff(x)), 50)\n",
    "    m = m - 1\n",
    "    m[m <= 0] = np.inf\n",
    "    m[:int(len(m) * (1 - max_regions_percentage))] = np.inf\n",
    "    idx_th = np.argmin(m) + 1\n",
    "\n",
    "    ###\n",
    "    ###plot selection\n",
    "    pl.figure()\n",
    "    pl.title('Selection of the HPRs')\n",
    "    pl.plot(x, y, 'r', lw=3)\n",
    "    pl.plot(x[idx_th], y[idx_th], '*', markersize=20)\n",
    "    x_ext = np.linspace(-0.1, 1.2, N_POINTS)\n",
    "    y_line = (m[idx_th] + 1.0) * (x_ext - x[idx_th]) + y[idx_th];\n",
    "    pl.plot(x_ext, y_line, '--k', lw=3)\n",
    "    pl.xlim(0, 1.1)\n",
    "    pl.ylim(0, 1)\n",
    "    pl.xlabel('Fraction of bins')\n",
    "    pl.ylabel('Score normalized')\n",
    "    pl.savefig(os.path.join(output_directory, 'SELECTION_OF_VARIABILITY_HOTSPOT.pdf'))\n",
    "    pl.close()\n",
    "\n",
    "    # print idx_th,\n",
    "    th_iod = sorted(iod_values)[idx_th]\n",
    "    # print th_iod\n",
    "\n",
    "    hpr_idxs = iod_values > th_iod\n",
    "\n",
    "    hpr_iod_scores = iod_values[hpr_idxs]\n",
    "    # print len(iod_values),len(hpr_idxs),sum(hpr_idxs), sum(hpr_idxs)/float(len(hpr_idxs)),\n",
    "\n",
    "    info('Selected %f%% regions (%d)' % (sum(hpr_idxs) / float(len(hpr_idxs)) * 100, sum(hpr_idxs)))\n",
    "    coordinates_bin['iod'] = iod_values\n",
    "    # we remove the regions \"without\" signal in any of the cell types\n",
    "    coordinates_bin.dropna(inplace=True)\n",
    "\n",
    "    df_chip_hpr_zscore = df_chip_not_empty.loc[hpr_idxs, :].apply(zscore, axis=1)\n",
    "\n",
    "    return hpr_idxs, coordinates_bin, df_chip_hpr_zscore, hpr_iod_scores\n",
    "\n",
    "\n",
    "def hpr_to_bigwig(coordinates_bin, tracks_directory, chr_len_filename):\n",
    "    bedgraph_iod_track_filename = os.path.join(tracks_directory,\n",
    "                                               'VARIABILITY.bedgraph')\n",
    "\n",
    "    bw_iod_track_filename = os.path.join(tracks_directory,\n",
    "                                         'VARIABILITY.bw')\n",
    "\n",
    "    # create a track for IGV\n",
    "\n",
    "    if not os.path.exists(bw_iod_track_filename) or recompute_all:\n",
    "\n",
    "        info('Generating variability track in bigwig format in:%s' % bw_iod_track_filename)\n",
    "\n",
    "        coordinates_bin.to_csv(bedgraph_iod_track_filename,\n",
    "                               sep='\\t',\n",
    "                               header=False,\n",
    "                               index=False)\n",
    "        sb.call('bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_iod_track_filename,\n",
    "                                                     chr_len_filename,\n",
    "                                                     bw_iod_track_filename),\n",
    "                shell=True)\n",
    "        try:\n",
    "            os.remove(bedgraph_iod_track_filename)\n",
    "        except:\n",
    "            pass\n",
    "    return bw_iod_track_filename\n",
    "\n",
    "\n",
    "def hpr_to_bedgraph(hpr_idxs, coordinates_bin, tracks_directory):\n",
    "    bedgraph_hpr_filename = os.path.join(tracks_directory,\n",
    "                                         'SELECTED_VARIABILITY_HOTSPOT.bedgraph')\n",
    "\n",
    "    bed_hpr_filename = os.path.join(output_directory,\n",
    "                                    'SELECTED_VARIABILITY_HOTSPOT.bed')\n",
    "\n",
    "    to_write = coordinates_bin.loc[hpr_idxs[hpr_idxs].index]\n",
    "    to_write.dropna(inplace=True)\n",
    "    to_write['bpstart'] = to_write['bpstart'].astype(int)\n",
    "    to_write['bpend'] = to_write['bpend'].astype(int)\n",
    "\n",
    "    to_write.to_csv(bedgraph_hpr_filename,\n",
    "                    sep='\\t',\n",
    "                    header=False,\n",
    "                    index=False)\n",
    "\n",
    "    if not os.path.exists(bed_hpr_filename) or recompute_all:\n",
    "        info('Writing the HPRs in: \"%s\"' % bed_hpr_filename)\n",
    "        sb.call('sort -k1,1 -k2,2n \"%s\" | bedtools merge -i stdin >  \"%s\"' % (bedgraph_hpr_filename,\n",
    "                                                                              bed_hpr_filename),\n",
    "                shell=True)\n",
    "\n",
    "    return bed_hpr_filename\n",
    "\n",
    "\n",
    "def write_specific_regions(coordinates_bin, df_chip_hpr_zscore, specific_regions_directory, depleted, z_score_low,\n",
    "                           z_score_high):\n",
    "    # write target\n",
    "    info('Writing Specific Regions for each cell line...')\n",
    "    coord_zscore = coordinates_bin.copy()\n",
    "    for col in df_chip_hpr_zscore:\n",
    "        regions_specific_filename = 'Regions_specific_for_%s_z_%.2f.bedgraph' % (\n",
    "            os.path.basename(col).replace('.rpm', ''), z_score_high)\n",
    "        specific_output_filename = os.path.join(specific_regions_directory,\n",
    "                                                regions_specific_filename)\n",
    "        specific_output_bed_filename = specific_output_filename.replace('.bedgraph', '.bed')\n",
    "\n",
    "        if not os.path.exists(specific_output_bed_filename) or recompute_all:\n",
    "            if depleted:\n",
    "                z_score_high = -z_score_high\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] < z_score_high, col]\n",
    "            else:\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] > z_score_high, col]\n",
    "            coord_zscore.dropna().to_csv(specific_output_filename,\n",
    "                                         sep='\\t',\n",
    "                                         header=False,\n",
    "                                         index=False)\n",
    "\n",
    "            info('Writing:%s' % specific_output_bed_filename)\n",
    "            sb.call('sort -k1,1 -k2,2n \"%s\" | bedtools merge -i stdin >  \"%s\"' % (specific_output_filename,\n",
    "                                                                                  specific_output_bed_filename),\n",
    "                    shell=True)\n",
    "    # write background\n",
    "    info('Writing Background Regions for each cell line...')\n",
    "    coord_zscore = coordinates_bin.copy()\n",
    "    for col in df_chip_hpr_zscore:\n",
    "\n",
    "        bg_output_filename = os.path.join(specific_regions_directory,\n",
    "                                          'Background_for_%s_z_%.2f.bedgraph' % (\n",
    "                                              os.path.basename(col).replace('.rpm', ''), z_score_low))\n",
    "        bg_output_bed_filename = bg_output_filename.replace('.bedgraph', '.bed')\n",
    "\n",
    "        if not os.path.exists(bg_output_bed_filename) or recompute_all:\n",
    "\n",
    "            if depleted:\n",
    "                z_score_low = -z_score_low\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] > z_score_low, col]\n",
    "            else:\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] < z_score_low, col]\n",
    "\n",
    "            coord_zscore.dropna().to_csv(bg_output_filename,\n",
    "                                         sep='\\t',\n",
    "                                         header=False,\n",
    "                                         index=False)\n",
    "\n",
    "            info('Writing:%s' % bg_output_bed_filename)\n",
    "            sb.call(\n",
    "                'sort -k1,1 -k2,2n -i \"%s\" | bedtools merge -i stdin >  \"%s\"' % (bg_output_filename,\n",
    "                                                                                 bg_output_bed_filename),\n",
    "                shell=True)\n",
    "\n",
    "\n",
    "def create_igv_track_file(hpr_iod_scores, bed_hpr_filename, genome_name, output_directory, binned_sample_names,\n",
    "                          disable_quantile_normalization):\n",
    "    igv_session_filename = os.path.join(output_directory, 'OPEN_ME_WITH_IGV.xml')\n",
    "    info('Creating an IGV session file (.xml) in: %s' % igv_session_filename)\n",
    "\n",
    "    session = ET.Element(\"Session\")\n",
    "    session.set(\"genome\", genome_name)\n",
    "    session.set(\"hasGeneTrack\", \"true\")\n",
    "    session.set(\"version\", \"7\")\n",
    "    resources = ET.SubElement(session, \"Resources\")\n",
    "    panel = ET.SubElement(session, \"Panel\")\n",
    "\n",
    "    resource_items = []\n",
    "    track_items = []\n",
    "\n",
    "    min_h = np.mean(hpr_iod_scores) - 2 * np.std(hpr_iod_scores)\n",
    "    max_h = np.mean(hpr_iod_scores) + 2 * np.std(hpr_iod_scores)\n",
    "    mid_h = np.mean(hpr_iod_scores)\n",
    "    # write the tracks\n",
    "    for binned_sample_name in binned_sample_names:\n",
    "        if disable_quantile_normalization:\n",
    "            track_full_path = os.path.join(output_directory, 'TRACKS', '%s.bw' % binned_sample_name)\n",
    "        else:\n",
    "            track_full_path = os.path.join(output_directory, 'TRACKS',\n",
    "                                           '%s_quantile_normalized.bw' % binned_sample_name)\n",
    "\n",
    "        track_filename = rem_base_path(track_full_path, output_directory)\n",
    "\n",
    "        if os.path.exists(track_full_path):\n",
    "            resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "            resource_items[-1].set(\"path\", track_filename)\n",
    "            track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "            track_items[-1].set('color', \"0,0,178\")\n",
    "            track_items[-1].set('id', track_filename)\n",
    "            track_items[-1].set(\"name\", binned_sample_name)\n",
    "\n",
    "    resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "    resource_items[-1].set(\"path\", rem_base_path(bw_iod_track_filename, output_directory))\n",
    "\n",
    "    track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "    track_items[-1].set('color', \"178,0,0\")\n",
    "    track_items[-1].set('id', rem_base_path(bw_iod_track_filename, output_directory))\n",
    "    track_items[-1].set('renderer', \"HEATMAP\")\n",
    "    track_items[-1].set(\"colorScale\",\n",
    "                        \"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (\n",
    "                            mid_h, min_h, mid_h, max_h))\n",
    "    track_items[-1].set(\"name\", 'VARIABILITY')\n",
    "\n",
    "    resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "    resource_items[-1].set(\"path\", rem_base_path(bed_hpr_filename, output_directory))\n",
    "    track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "    track_items[-1].set('color', \"178,0,0\")\n",
    "    track_items[-1].set('id', rem_base_path(bed_hpr_filename, output_directory))\n",
    "    track_items[-1].set('renderer', \"HEATMAP\")\n",
    "    track_items[-1].set(\"colorScale\",\n",
    "                        \"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (\n",
    "                            mid_h, min_h, mid_h, max_h))\n",
    "    track_items[-1].set(\"name\", 'HOTSPOTS')\n",
    "\n",
    "    for sample_name in sample_names:\n",
    "        track_full_path = \\\n",
    "            glob.glob(os.path.join(output_directory, 'SPECIFIC_REGIONS',\n",
    "                                   'Regions_specific_for_%s*.bedgraph' % sample_name))[0]\n",
    "        specific_track_filename = rem_base_path(track_full_path, output_directory)\n",
    "        if os.path.exists(track_full_path):\n",
    "            resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "            resource_items[-1].set(\"path\", specific_track_filename)\n",
    "\n",
    "            track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "            track_items[-1].set('color', \"178,0,0\")\n",
    "            track_items[-1].set('id', specific_track_filename)\n",
    "            track_items[-1].set('renderer', \"HEATMAP\")\n",
    "            track_items[-1].set(\"colorScale\",\n",
    "                                \"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (\n",
    "                                    mid_h, min_h, mid_h, max_h))\n",
    "            track_items[-1].set(\"name\", 'REGION SPECIFIC FOR %s' % sample_name)\n",
    "\n",
    "    tree = ET.ElementTree(session)\n",
    "    tree.write(igv_session_filename, xml_declaration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[H A Y S T A C K   H O T S P O T]\n",
      "\n",
      "-SELECTION OF VARIABLE REGIONS- [Luca Pinello - lpinello@jimmy.harvard.edu]\n",
      "\n",
      "Version 0.4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print '\\n[H A Y S T A C K   H O T S P O T]'\n",
    "print('\\n-SELECTION OF VARIABLE REGIONS- [Luca Pinello - lpinello@jimmy.harvard.edu]\\n')\n",
    "print 'Version %s\\n' % HAYSTACK_VERSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Intialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Mon, 24 Jul 2017 18:47:04:\n",
      "\t {'input_is_bigwig': False, 'th_rpm': 99, 'samples_filename': '/mnt/hd2/test_data/samples_names.txt', 'name': '', 'max_regions_percentage': 0.1, 'disable_quantile_normalization': False, 'z_score_low': 0.25, 'output_directory': '/mnt/hd2/test_data/OUTPUT7', 'blacklist': False, 'bin_size': 200, 'n_processes': 8, 'depleted': False, 'read_ext': 200, 'genome_name': 'hg19', 'z_score_high': 1.5, 'chrom_exclude': '', 'transformation': 'angle', 'recompute_all': False} \n",
      "\n",
      "INFO  @ Mon, 24 Jul 2017 18:47:04:\n",
      "\t Initializing Genome:hg19 \n",
      "\n",
      "INFO  @ Mon, 24 Jul 2017 18:47:05:\n",
      "\t Checking samples files location... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1.1\n",
    "check_required_packages()\n",
    "# step 1.2\n",
    "parser = get_args()\n",
    "\n",
    "input_args=['/mnt/hd2/test_data/samples_names.txt',\n",
    "            'hg19',\n",
    "            '--output_directory',\n",
    "            '/mnt/hd2/test_data/OUTPUT7',\n",
    "            '--bin_size',\n",
    "            '200',\n",
    "             '--chrom_exclude',\n",
    "             '']\n",
    "\n",
    "\n",
    "args = parser.parse_args(input_args)\n",
    "info(vars(args))\n",
    "\n",
    "global recompute_all\n",
    "recompute_all = args.recompute_all\n",
    "\n",
    "# step 1.3: create directories\n",
    "if args.name:\n",
    "    directory_name = 'HAYSTACK_HOTSPOTS_on_%s' % args.name\n",
    "else:\n",
    "    directory_name = 'HAYSTACK_HOTSPOTS'\n",
    "if args.output_directory:\n",
    "    output_directory = os.path.join(args.output_directory, directory_name)\n",
    "else:\n",
    "    output_directory = directory_name\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "tracks_directory = os.path.join(output_directory, 'TRACKS')\n",
    "if not os.path.exists(tracks_directory):\n",
    "    os.makedirs(tracks_directory)\n",
    "intermediate_directory = os.path.join(output_directory, 'INTERMEDIATE')\n",
    "if not os.path.exists(intermediate_directory):\n",
    "    os.makedirs(intermediate_directory)\n",
    "specific_regions_directory = os.path.join(output_directory, 'SPECIFIC_REGIONS')\n",
    "if not os.path.exists(specific_regions_directory):\n",
    "    os.makedirs(specific_regions_directory)\n",
    "\n",
    "# step 1.4: get genome data\n",
    "chr_len_filename = initialize_genome(args.genome_name)\n",
    "\n",
    "# step 1.5: get filepaths\n",
    "sample_names, data_filenames = get_data_filepaths(args.samples_filename)\n",
    "binned_sample_names = ['%s.%dbp' % (sample_name, args.bin_size) for sample_name in sample_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create tiled genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Mon, 24 Jul 2017 18:54:33:\n",
      "\t Creating bins of 200bp in /mnt/hd2/test_data/OUTPUT7/HAYSTACK_HOTSPOTS/hg19.200bp.bins.sorted.bed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "genome_sorted_bins_file = create_tiled_genome(args.genome_name,\n",
    "                                              output_directory,\n",
    "                                              chr_len_filename,\n",
    "                                              args.bin_size,\n",
    "                                              args.chrom_exclude,\n",
    "                                              args.blacklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert files to genome-wide rpm tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.input_is_bigwig:\n",
    "    bigwig_filenames = copy_bigwigs(data_filenames,\n",
    "                                    sample_names,\n",
    "                                    tracks_directory)\n",
    "    binned_rpm_filenames = to_binned_tracks_if_bigwigs(bigwig_filenames,\n",
    "                                                       intermediate_directory,\n",
    "                                                       binned_sample_names,\n",
    "                                                       genome_sorted_bins_file)\n",
    "else:\n",
    "    bam_filtered_nodup_filenames = to_filtered_deduped_bams(data_filenames,\n",
    "                                                            output_directory,\n",
    "                                                            args.n_processes)\n",
    "\n",
    "    bedgraph_filenames, bigwig_filenames = to_normalized_extended_reads_tracks(bam_filtered_nodup_filenames,\n",
    "                                                                               sample_names,\n",
    "                                                                               tracks_directory,\n",
    "                                                                               chr_len_filename,\n",
    "                                                                               args.read_ext)\n",
    "    binned_rpm_filenames = to_binned_tracks(bedgraph_filenames,\n",
    "                                            binned_sample_names,\n",
    "                                            tracks_directory,\n",
    "                                            intermediate_directory,\n",
    "                                            chr_len_filename,\n",
    "                                            genome_sorted_bins_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create dataframe and perform quantile normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_chip = load_binned_rpm_tracks(binned_sample_names, binned_rpm_filenames)\n",
    "coordinates_bin = pd.read_csv(genome_sorted_bins_file,\n",
    "                              names=['chr_id', 'bpstart', 'bpend'],\n",
    "                              sep='\\t',\n",
    "                              header=None,\n",
    "                              usecols=[0, 1, 2])\n",
    "\n",
    "if not args.disable_quantile_normalization:\n",
    "    info('Normalizing the data...')\n",
    "\n",
    "    df_chip, bigwig_binned_normalized_filenames = to_binned_normalized_tracks(df_chip,\n",
    "                                                                              coordinates_bin,\n",
    "                                                                              binned_sample_names,\n",
    "                                                                              chr_len_filename,\n",
    "                                                                              tracks_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Determine HP regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 5\n",
    "info('Determine HP regions')\n",
    "hpr_idxs, coordinates_bin, df_chip_hpr_zscore, hpr_iod_scores = find_hpr_coordinates(df_chip,\n",
    "                                                                                     coordinates_bin,\n",
    "                                                                                     args.th_rpm,\n",
    "                                                                                     args.transformation,\n",
    "                                                                                     args.max_regions_percentage)\n",
    "info('hpr to bigwig')\n",
    "bw_iod_track_filename = hpr_to_bigwig(coordinates_bin,\n",
    "                                      tracks_directory,\n",
    "                                      chr_len_filename)\n",
    "info('hpr to bedgraph')\n",
    "bed_hpr_filename = hpr_to_bedgraph(hpr_idxs,\n",
    "                                   coordinates_bin,\n",
    "                                   tracks_directory)\n",
    "info('Save files')\n",
    "write_specific_regions(coordinates_bin,\n",
    "                       df_chip_hpr_zscore,\n",
    "                       specific_regions_directory,\n",
    "                       args.depleted,\n",
    "                       args.z_score_low,\n",
    "                       args.z_score_high)\n",
    "create_igv_track_file(hpr_iod_scores,\n",
    "                      bed_hpr_filename,\n",
    "                      args.genome_name,\n",
    "                      output_directory,\n",
    "                      binned_sample_names,\n",
    "                      args.disable_quantile_normalization)\n",
    "\n",
    "info('All done! Ciao!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
