{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haystack Hotspots Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import subprocess as sb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "from pybedtools import BedTool\n",
    "import multiprocessing\n",
    "# commmon functions\n",
    "from haystack_common import determine_path, which, check_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create logging file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAYSTACK_VERSION = \"0.4.0\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(levelname)-5s @ %(asctime)s:\\n\\t %(message)s \\n',\n",
    "                    datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                    stream=sys.stderr,\n",
    "                    filemode=\"w\"\n",
    "                    )\n",
    "error = logging.critical\n",
    "warn = logging.warning\n",
    "debug = logging.debug\n",
    "info = logging.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantile_normalization(A):\n",
    "    AA = np.zeros_like(A)\n",
    "    I = np.argsort(A, axis=0)\n",
    "    AA[I, np.arange(A.shape[1])] = np.mean(A[I, np.arange(A.shape[1])], axis=1)[:, np.newaxis]\n",
    "    return AA\n",
    "\n",
    "\n",
    "def smooth(x, window_len=200):\n",
    "    s = np.r_[x[window_len - 1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.hanning(window_len)\n",
    "    y = np.convolve(w / w.sum(), s, mode='valid')\n",
    "    return y[int(window_len / 2):-int(window_len / 2) + 1]\n",
    "\n",
    "\n",
    "# write the IGV session file\n",
    "def rem_base_path(path, base_path):\n",
    "    return path.replace(os.path.join(base_path, ''), '')\n",
    "\n",
    "\n",
    "def find_th_rpm(df_chip, th_rpm):\n",
    "    return np.min(df_chip.apply(lambda x: np.percentile(x, th_rpm)))\n",
    "\n",
    "\n",
    "def log2_transform(x):\n",
    "    return np.log2(x + 1)\n",
    "\n",
    "\n",
    "def angle_transform(x):\n",
    "    return np.arcsin(np.sqrt(x) / 1000000.0)\n",
    "\n",
    "\n",
    "# def normalize_count(feature, scaling_factor):\n",
    "#     feature.name = str(int(feature.name) * scaling_factor)\n",
    "#     return feature\n",
    "\n",
    "def get_scaling_factor(bam_filename):\n",
    "    from pysam import AlignmentFile\n",
    "    \n",
    "    infile = AlignmentFile(bam_filename, \"rb\")\n",
    "    numreads = infile.count(until_eof=True)\n",
    "    scaling_factor = (1.0 / float(numreads)) * 1000000\n",
    "    scaling_factor =np.float32(scaling_factor)\n",
    "    return scaling_factor\n",
    "\n",
    "\n",
    "def check_required_packages():\n",
    "    if which('samtools') is None:\n",
    "        error(\n",
    "            'Haystack requires samtools. '\n",
    "            'Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('bedtools') is None:\n",
    "        error('Haystack requires bedtools.'\n",
    "              ' Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('bedGraphToBigWig') is None:\n",
    "        info(\n",
    "            ' Haystack requires bedGraphToBigWig.'\n",
    "            ' Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('sambamba') is None:\n",
    "        info(\n",
    "            'Haystack requires sambamba.'\n",
    "            ' Please install using bioconda')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if which('bigWigAverageOverBed') is None:\n",
    "        info(\n",
    "            'Haystack requires bigWigAverageOverBed. '\n",
    "            'Please install using bioconda')\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create parse argument function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # mandatory\n",
    "    parser = argparse.ArgumentParser(description='HAYSTACK Parameters')\n",
    "    parser.add_argument('samples_filename',\n",
    "                        type=str,\n",
    "                        help='A tab delimited file with in each row (1) a sample name, '\n",
    "                             '(2) the path to the corresponding bam or bigwig filename')\n",
    "    parser.add_argument('genome_name',\n",
    "                        type=str,\n",
    "                        help='Genome assembly to use from UCSC (for example hg19, mm9, etc.)')\n",
    "    # optional\n",
    "    parser.add_argument('--bin_size',\n",
    "                        type=int,\n",
    "                        help='bin size to use(default: 500bp)',\n",
    "                        default=500)\n",
    "    parser.add_argument('--disable_quantile_normalization',\n",
    "                        help='Disable quantile normalization (default: False)',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--th_rpm',\n",
    "                        type=float,\n",
    "                        help='Percentile on the signal intensity to consider for the hotspots (default: 99)',\n",
    "                        default=99)\n",
    "    parser.add_argument('--transformation',\n",
    "                        type=str,\n",
    "                        help='Variance stabilizing transformation among: none, log2, angle (default: angle)',\n",
    "                        default='angle',\n",
    "                        choices=['angle', 'log2', 'none'])\n",
    "    parser.add_argument('--recompute_all',\n",
    "                        help='Ignore any file previously precalculated',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--z_score_high',\n",
    "                        type=float,\n",
    "                        help='z-score value to select the specific regions(default: 1.5)',\n",
    "                        default=1.5)\n",
    "    parser.add_argument('--z_score_low',\n",
    "                        type=float,\n",
    "                        help='z-score value to select the not specific regions (default: 0.25)',\n",
    "                        default=0.25)\n",
    "    parser.add_argument('--name',\n",
    "                        help='Define a custom output filename for the report',\n",
    "                        default='')\n",
    "    parser.add_argument('--output_directory',\n",
    "                        type=str,\n",
    "                        help='Output directory (default: current directory)',\n",
    "                        default='')\n",
    "    parser.add_argument('--blacklist',\n",
    "                        help='Exclude blacklisted regions.',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--chrom_exclude',\n",
    "                        help='Exclude chromosomes. For example (_|chrM|chrX|chrY).',\n",
    "                        default='chrX|chrY')\n",
    "    parser.add_argument('--read_ext',\n",
    "                        type=int,\n",
    "                        help='Read extension in bps (default: 200)',\n",
    "                        default=200)\n",
    "    parser.add_argument('--max_regions_percentage',\n",
    "                        type=float,\n",
    "                        help='Upper bound on the %% of the regions selected  (default: 0.1, 0.0=0%% 1.0=100%%)',\n",
    "                        default=0.1)\n",
    "    parser.add_argument('--depleted',\n",
    "                        help='Look for cell type specific regions with depletion of signal instead of enrichment',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--input_is_bigwig',\n",
    "                        help='Use the bigwig format instead of the bam format for the input. '\n",
    "                             'Note: The files must have extension .bw',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--n_processes',\n",
    "                        type=int,\n",
    "                        help='Specify the number of processes to use. The default is #cores available.',\n",
    "                        default=multiprocessing.cpu_count())\n",
    "    parser.add_argument('--version',\n",
    "                        help='Print version and exit.',\n",
    "                        action='version',\n",
    "                        version='Version %s' % HAYSTACK_VERSION)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_filepaths(samples_filename):\n",
    "    # check folder or sample filename\n",
    "    if os.path.isfile(samples_filename):\n",
    "        data_filenames = []\n",
    "        sample_names = []\n",
    "        with open(samples_filename) as infile:\n",
    "            for line in infile:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                if line.startswith('#'): # skip optional header line\n",
    "                    info('Skipping header/comment line:%s' % line)\n",
    "                    continue\n",
    "\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                n_fields = len(fields)\n",
    "\n",
    "                if n_fields == 2:\n",
    "                    sample_names.append(fields[0])\n",
    "                    data_filenames.append(fields[1])\n",
    "                else:\n",
    "                    error('The samples file format is wrong!')\n",
    "                    sys.exit(1)\n",
    "\n",
    "    dir_path = os.path.dirname(os.path.realpath(samples_filename))\n",
    "    data_filenames = [os.path.join(dir_path, filename) \n",
    "                      for filename in data_filenames]\n",
    "    # check all the files before starting\n",
    "    info('Checking samples files location...')\n",
    "    for data_filename in data_filenames:\n",
    "        check_file(data_filename)\n",
    "\n",
    "    return sample_names, data_filenames\n",
    "\n",
    "def initialize_genome(genome_name):\n",
    "    from bioutilities import Genome_2bit\n",
    "\n",
    "    info('Initializing Genome:%s' % genome_name)\n",
    "\n",
    "    genome_directory = determine_path('genomes')\n",
    "\n",
    "    genome_2bit = os.path.join(genome_directory,\n",
    "                               genome_name + '.2bit')\n",
    "    chr_len_filename = os.path.join(genome_directory,\n",
    "                                    \"%s_chr_lengths.txt\" % genome_name)\n",
    "    if os.path.exists(genome_2bit):\n",
    "        Genome_2bit(genome_2bit)\n",
    "    else:\n",
    "        info(\"\\nIt seems you don't have the required genome file.\")\n",
    "\n",
    "        download_genome(genome_name,\n",
    "                        genome_directory)\n",
    "        if os.path.exists(genome_2bit):\n",
    "            info('Genome correctly downloaded!')\n",
    "            Genome_2bit(genome_2bit)\n",
    "        else:\n",
    "            error('Sorry I cannot download the required file for you.'\n",
    "                  ' Check your Internet connection.')\n",
    "            sys.exit(1)\n",
    "\n",
    "    check_file(chr_len_filename)\n",
    "\n",
    "    return chr_len_filename\n",
    "\n",
    "def create_tiled_genome(genome_name,\n",
    "                        output_directory,\n",
    "                        chr_len_filename,\n",
    "                        bin_size,\n",
    "                        chrom_exclude,\n",
    "                        blacklist):\n",
    "\n",
    "    from re import search\n",
    "    genome_directory = determine_path('genomes')\n",
    "\n",
    "    genome_sorted_bins_file = os.path.join(output_directory, '%s.%dbp.bins.sorted.bed'\n",
    "                                           % (os.path.basename(genome_name), bin_size))\n",
    "\n",
    "    if not os.path.exists(genome_sorted_bins_file) or recompute_all:\n",
    "\n",
    "        info('Creating bins of %dbp in %s' % (bin_size, genome_sorted_bins_file))\n",
    "\n",
    "        if chrom_exclude:\n",
    "            chr_len_filtered_filename = os.path.join(genome_directory,\n",
    "                                                     \"%s_chr_lengths_filtered.txt\" % genome_name)\n",
    "\n",
    "            with open(chr_len_filtered_filename, 'wb') as f:\n",
    "                f.writelines(line for line in open(chr_len_filename)\n",
    "                             if not search(chrom_exclude, line.split()[0]))\n",
    "        else:\n",
    "            chr_len_filtered_filename = chr_len_filename\n",
    "\n",
    "        tiled_genome = BedTool(). \\\n",
    "            window_maker(g=chr_len_filtered_filename,\n",
    "                         w=bin_size).sort()\n",
    "\n",
    "        if blacklist:\n",
    "\n",
    "            blacklist_filepath = os.path.join(genome_directory,\n",
    "                                              'blacklist.bed')\n",
    "            check_file(blacklist_filepath)\n",
    "\n",
    "            tiled_genome.intersect(blacklist_filepath,\n",
    "                                   wa=True,\n",
    "                                   v=True,\n",
    "                                   output=genome_sorted_bins_file)\n",
    "        else:\n",
    "            tiled_genome.saveas(genome_sorted_bins_file)\n",
    "    return genome_sorted_bins_file\n",
    "\n",
    "### if bigwig\n",
    "def copy_bigwigs(data_filenames, sample_names, tracks_directory):\n",
    "    import shutil\n",
    "    bigwig_filenames = [os.path.join(tracks_directory,\n",
    "                                     '%s.bw' % sample_name)\n",
    "                        for sample_name in sample_names]\n",
    "\n",
    "    for data_filename, bigwig_filename in zip(data_filenames,\n",
    "                                              bigwig_filenames):\n",
    "        shutil.copy2(data_filename, bigwig_filename)\n",
    "\n",
    "    return bigwig_filenames\n",
    "\n",
    "def to_binned_tracks_if_bigwigs(data_filenames,\n",
    "                                intermediate_directory,\n",
    "                                binned_sample_names,\n",
    "                                genome_sorted_bins_file):\n",
    "\n",
    "    binned_rpm_filenames = [os.path.join(intermediate_directory,\n",
    "                                         '%s.rpm' % binned_sample_name)\n",
    "                            for binned_sample_name in binned_sample_names]\n",
    "\n",
    "    for data_filename, binned_rpm_filename in zip(data_filenames,\n",
    "                                                  binned_rpm_filenames):\n",
    "\n",
    "        if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "            info('Processing:%s' % data_filename)\n",
    "\n",
    "            cmd = 'bigWigAverageOverBed %s %s  /dev/stdout |' \\\n",
    "                  ' sort -s -n -k 1,1 | cut -f5 > %s' % (data_filename,\n",
    "                                                         genome_sorted_bins_file,\n",
    "                                                         binned_rpm_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "    return binned_rpm_filenames\n",
    "#######\n",
    "# convert bam files to genome-wide rpm tracks\n",
    "def to_filtered_deduped_bams(bam_filenames,\n",
    "                             output_directory,\n",
    "                             n_processes):\n",
    "\n",
    "    filtered_bam_directory = os.path.join(output_directory, 'FILTERED_BAMS')\n",
    "\n",
    "    if not os.path.exists(filtered_bam_directory):\n",
    "        os.makedirs(filtered_bam_directory)\n",
    "\n",
    "    bam_filtered_nodup_filenames = [os.path.join(\n",
    "        filtered_bam_directory,\n",
    "        '%s.filtered.nodup%s' % (os.path.splitext(os.path.basename(bam_filename))))\n",
    "        for bam_filename in bam_filenames]\n",
    "\n",
    "    for bam_filename, bam_filtered_nodup_filename in zip(bam_filenames,\n",
    "                                                         bam_filtered_nodup_filenames):\n",
    "\n",
    "        if not os.path.exists(bam_filtered_nodup_filename) or recompute_all:\n",
    "\n",
    "            info('Processing:%s' % bam_filename)\n",
    "            bam_temp_filename = os.path.join(os.path.dirname(bam_filtered_nodup_filename),\n",
    "                                             '%s.temp%s' % os.path.splitext(\n",
    "                                                 os.path.basename(bam_filtered_nodup_filename)))\n",
    "            info('Removing  unmapped, mate unmapped, not primary alignment,'\n",
    "                 ' low MAPQ reads, and reads failing qc')\n",
    "\n",
    "            # cmd = 'sambamba view -f bam -l 0 -t %d -F \"not (unmapped or mate_is_unmapped or failed_quality_control or duplicate or secondary_alignment) and mapping_quality >= 30\" \"%s\"  -o \"%s\"' % (\n",
    "            #     n_processes, bam_filename, bam_temp_filename)\n",
    "\n",
    "            cmd = 'sambamba view -f bam -l 0 -t %d -F \"not failed_quality_control\" \"%s\"  -o \"%s\"' % (n_processes,\n",
    "                                                                                                     bam_filename,\n",
    "                                                                                                     bam_temp_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "            info('Removing  optical duplicates')\n",
    "            cmd = 'sambamba markdup  -l 5 -t %d --hash-table-size=17592186044416' \\\n",
    "                  ' --overflow-list-size=20000000 --io-buffer-size=256 \"%s\" \"%s\" ' % (\n",
    "                n_processes,\n",
    "                bam_temp_filename,\n",
    "                bam_filtered_nodup_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "            try:\n",
    "                os.remove(bam_temp_filename)\n",
    "                os.remove(bam_temp_filename + '.bai')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return bam_filtered_nodup_filenames\n",
    "\n",
    "def to_normalized_extended_reads_tracks(bam_filenames,\n",
    "                                        sample_names,\n",
    "                                        tracks_directory,\n",
    "                                        chr_len_filename,\n",
    "                                        read_ext):\n",
    "\n",
    "    bedgraph_filenames = [os.path.join(tracks_directory, '%s.bedgraph' % sample_name)\n",
    "                          for sample_name in sample_names]\n",
    "    bigwig_filenames = [filename.replace('.bedgraph', '.bw')\n",
    "                        for filename in bedgraph_filenames]\n",
    "\n",
    "    for bam_filename, bedgraph_filename, bigwig_filename in zip(bam_filenames,\n",
    "                                                                bedgraph_filenames,\n",
    "                                                                bigwig_filenames):\n",
    "\n",
    "        if not os.path.exists(bedgraph_filename) or recompute_all:\n",
    "            info('Processing:%s' % bam_filename)\n",
    "\n",
    "            info('Computing Scaling Factor...')\n",
    "            scaling_factor = get_scaling_factor(bam_filename)\n",
    "            info('Scaling Factor: %e' % scaling_factor)\n",
    "\n",
    "            info('Converting bam to bed and extending read length...')\n",
    "            BedTool(bam_filename). \\\n",
    "                bam_to_bed(). \\\n",
    "                slop(r=read_ext,\n",
    "                     l=0,\n",
    "                     s=True,\n",
    "                     g=chr_len_filename). \\\n",
    "                genome_coverage(bg=True,\n",
    "                                scale=scaling_factor,\n",
    "                                g=chr_len_filename). \\\n",
    "                sort(). \\\n",
    "                saveas(bedgraph_filename)\n",
    "\n",
    "        if not os.path.exists(bigwig_filename) or recompute_all:\n",
    "            info('Converting BedGraph to BigWig')\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_filename,\n",
    "                                                       chr_len_filename,\n",
    "                                                       bigwig_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "            info('Computing coverage over bins...')\n",
    "            info('Normalizing counts by scaling factor...')\n",
    "\n",
    "    return bedgraph_filenames, bigwig_filenames\n",
    "\n",
    "\n",
    "def to_binned_tracks(bedgraph_filenames,\n",
    "                     binned_sample_names,\n",
    "                     tracks_directory,\n",
    "                     intermediate_directory,\n",
    "                     chr_len_filename,\n",
    "                     genome_sorted_bins_file):\n",
    "\n",
    "    bedgraph_binned_filenames = [os.path.join(tracks_directory,\n",
    "                                              '%s.bedgraph' % binned_sample_name)\n",
    "                                 for binned_sample_name in binned_sample_names]\n",
    "\n",
    "    binned_rpm_filenames = [os.path.join(intermediate_directory,\n",
    "                                         '%s.rpm' % binned_sample_name)\n",
    "                            for binned_sample_name in binned_sample_names]\n",
    "\n",
    "    bigwig_binned_filenames = [filename.replace('.bedgraph', '.bw')\n",
    "                               for filename in bedgraph_binned_filenames]\n",
    "\n",
    "    for bedgraph_filename, bedgraph_binned_filename,\\\n",
    "        binned_rpm_filename, bigwig_binned_filename in zip(bedgraph_filenames,\n",
    "                                                           bedgraph_binned_filenames,\n",
    "                                                           binned_rpm_filenames,\n",
    "                                                           bigwig_binned_filenames):\n",
    "\n",
    "        if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "            info('Making constant binned rpm values file: %s' % binned_rpm_filename)\n",
    "            bedgraph = BedTool(genome_sorted_bins_file). \\\n",
    "                map(b=bedgraph_filename,\n",
    "                    c=4,\n",
    "                    o='mean',\n",
    "                    null=0.0).\\\n",
    "                saveas(bedgraph_binned_filename)\n",
    "\n",
    "            bedgraph.to_dataframe()['name'].\\\n",
    "                to_csv(binned_rpm_filename,\n",
    "                       index=False)\n",
    "\n",
    "            # bedgraph = BedTool(genome_sorted_bins_file). \\\n",
    "            #     intersect(bed_extended, c=True). \\\n",
    "            #     each(normalize_count, scaling_factor). \\\n",
    "            #     saveas(bedgraph_filename)\n",
    "\n",
    "            info('Binned Bedgraph saved...')\n",
    "\n",
    "        if not os.path.exists(bigwig_binned_filename) or recompute_all:\n",
    "            info('Converting BedGraph to BigWig')\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_binned_filename,\n",
    "                                                       chr_len_filename,\n",
    "                                                       bigwig_binned_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "    return binned_rpm_filenames\n",
    "\n",
    "def load_binned_rpm_tracks(binned_sample_names, binned_rpm_filenames):\n",
    "    # load all the tracks\n",
    "    info('Loading the processed tracks')\n",
    "    df_chip = {}\n",
    "    for binned_sample_name, binned_rpm_filename in zip(binned_sample_names,\n",
    "                                                       binned_rpm_filenames):\n",
    "\n",
    "        df_chip[binned_sample_name] = pd.read_csv(binned_rpm_filename,\n",
    "                                                  squeeze=True,\n",
    "                                                  header=None)\n",
    "\n",
    "        info('Loading %s from file %s' % (binned_sample_name,\n",
    "                                          binned_rpm_filename))\n",
    "\n",
    "    df_chip = pd.DataFrame(df_chip)\n",
    "\n",
    "    return df_chip\n",
    "\n",
    "def to_binned_normalized_tracks(df_chip,\n",
    "                                coordinates_bin,\n",
    "                                binned_sample_names,\n",
    "                                chr_len_filename,\n",
    "                                tracks_directory):\n",
    "\n",
    "    df_chip_normalized = pd.DataFrame(quantile_normalization(df_chip.values),\n",
    "                                      columns=df_chip.columns,\n",
    "                                      index=df_chip.index)\n",
    "\n",
    "    bedgraph_binned_normalized_filenames = [os.path.join(tracks_directory,\n",
    "                                                         '%s_quantile_normalized.bedgraph' % binned_sample_name)\n",
    "                                            for binned_sample_name in binned_sample_names]\n",
    "    bigwig_binned_normalized_filenames = [filename.replace('.bedgraph', '.bw')\n",
    "                                          for filename in bedgraph_binned_normalized_filenames]\n",
    "    # write quantile normalized tracks\n",
    "    for binned_sample_name, bedgraph_binned_normalized_filename, bigwig_binned_normalized_filename in zip(\n",
    "            binned_sample_names,\n",
    "            bedgraph_binned_normalized_filenames,\n",
    "            bigwig_binned_normalized_filenames):\n",
    "\n",
    "        if not os.path.exists(bedgraph_binned_normalized_filename) or recompute_all:\n",
    "            info('Writing binned track: %s' % bigwig_binned_normalized_filename)\n",
    "            joined_df = pd.DataFrame.join(coordinates_bin, df_chip_normalized[binned_sample_name])\n",
    "            joined_df.to_csv(bedgraph_binned_normalized_filename,\n",
    "                             sep='\\t',\n",
    "                             header=False,\n",
    "                             index=False)\n",
    "\n",
    "        if not os.path.exists(bigwig_binned_normalized_filename) or recompute_all:\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_binned_normalized_filename,\n",
    "                                                       chr_len_filename,\n",
    "                                                       bigwig_binned_normalized_filename)\n",
    "            sb.call(cmd, shell=True)\n",
    "\n",
    "    return df_chip_normalized, bigwig_binned_normalized_filenames\n",
    "\n",
    "def find_hpr_coordinates(df_chip,\n",
    "                         coordinates_bin,\n",
    "                         th_rpm,\n",
    "                         transformation,\n",
    "                         max_regions_percentage,\n",
    "                         output_directory):\n",
    "\n",
    "    from scipy.stats import zscore\n",
    "    import matplotlib as mpl\n",
    "    mpl.use('Agg')\n",
    "    import pylab as pl\n",
    "\n",
    "    # th_rpm=args.th_rpm\n",
    "    # transformation=args.transformation\n",
    "    # max_regions_percentage=args.max_regions_percentage\n",
    "    # th_rpm=np.min(df_chip.apply(lambda x: np.percentile(x,th_rpm)))\n",
    "    th_rpm_est = find_th_rpm(df_chip, th_rpm)\n",
    "    info('Estimated th_rpm:%s' % th_rpm_est)\n",
    "\n",
    "    df_chip_not_empty = df_chip.loc[(df_chip > th_rpm_est).any(1), :]\n",
    "\n",
    "    if transformation == 'log2':\n",
    "        df_chip_not_empty = df_chip_not_empty.applymap(log2_transform)\n",
    "        info('Using log2 transformation')\n",
    "\n",
    "    elif transformation == 'angle':\n",
    "        df_chip_not_empty = df_chip_not_empty.applymap(angle_transform)\n",
    "        info('Using angle transformation')\n",
    "\n",
    "    else:\n",
    "        info('Using no transformation')\n",
    "\n",
    "    iod_values = df_chip_not_empty.var(1) / df_chip_not_empty.mean(1)\n",
    "\n",
    "    ####calculate the inflation point a la superenhancers\n",
    "    scores = iod_values\n",
    "    min_s = np.min(scores)\n",
    "    max_s = np.max(scores)\n",
    "\n",
    "    N_POINTS = len(scores)\n",
    "    x = np.linspace(0, 1, N_POINTS)\n",
    "    y = sorted((scores - min_s) / (max_s - min_s))\n",
    "    m = smooth((np.diff(y) / np.diff(x)), 50)\n",
    "    m = m - 1\n",
    "    m[m <= 0] = np.inf\n",
    "    m[:int(len(m) * (1 - max_regions_percentage))] = np.inf\n",
    "    idx_th = np.argmin(m) + 1\n",
    "\n",
    "    # print idx_th,\n",
    "    th_iod = sorted(iod_values)[idx_th]\n",
    "    # print th_iod\n",
    "\n",
    "    hpr_idxs = iod_values > th_iod\n",
    "\n",
    "    hpr_iod_scores = iod_values[hpr_idxs]\n",
    "    # print len(iod_values),len(hpr_idxs),sum(hpr_idxs), sum(hpr_idxs)/float(len(hpr_idxs)),\n",
    "\n",
    "    info('Selected %f%% regions (%d)' % (sum(hpr_idxs) / float(len(hpr_idxs)) * 100, sum(hpr_idxs)))\n",
    "    coordinates_bin['iod'] = iod_values\n",
    "    # we remove the regions \"without\" signal in any of the cell types\n",
    "    coordinates_bin.dropna(inplace=True)\n",
    "\n",
    "    df_chip_hpr_zscore = df_chip_not_empty.loc[hpr_idxs, :].apply(zscore, axis=1)\n",
    "\n",
    "\n",
    "    ###plot selection\n",
    "    pl.figure()\n",
    "    pl.title('Selection of the HPRs')\n",
    "    pl.plot(x, y, 'r', lw=3)\n",
    "    pl.plot(x[idx_th], y[idx_th], '*', markersize=20)\n",
    "    x_ext = np.linspace(-0.1, 1.2, N_POINTS)\n",
    "    y_line = (m[idx_th] + 1.0) * (x_ext - x[idx_th]) + y[idx_th];\n",
    "    pl.plot(x_ext, y_line, '--k', lw=3)\n",
    "    pl.xlim(0, 1.1)\n",
    "    pl.ylim(0, 1)\n",
    "    pl.xlabel('Fraction of bins')\n",
    "    pl.ylabel('Score normalized')\n",
    "    pl.savefig(os.path.join(output_directory,\n",
    "                            'SELECTION_OF_VARIABILITY_HOTSPOT.pdf'))\n",
    "    pl.close()\n",
    "\n",
    "    return hpr_idxs, coordinates_bin, df_chip_hpr_zscore, hpr_iod_scores\n",
    "\n",
    "\n",
    "def hpr_to_bigwig(coordinates_bin,\n",
    "                  tracks_directory,\n",
    "                  chr_len_filename):\n",
    "\n",
    "    bedgraph_iod_track_filename = os.path.join(tracks_directory,\n",
    "                                               'VARIABILITY.bedgraph')\n",
    "\n",
    "    bw_iod_track_filename = os.path.join(tracks_directory,\n",
    "                                         'VARIABILITY.bw')\n",
    "\n",
    "    # create a track for IGV\n",
    "\n",
    "    if not os.path.exists(bw_iod_track_filename) or recompute_all:\n",
    "\n",
    "        info('Generating variability track in bigwig format in:%s'\n",
    "             % bw_iod_track_filename)\n",
    "\n",
    "        coordinates_bin.to_csv(bedgraph_iod_track_filename,\n",
    "                               sep='\\t',\n",
    "                               header=False,\n",
    "                               index=False)\n",
    "        sb.call('bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (bedgraph_iod_track_filename,\n",
    "                                                     chr_len_filename,\n",
    "                                                     bw_iod_track_filename),\n",
    "                shell=True)\n",
    "        try:\n",
    "            os.remove(bedgraph_iod_track_filename)\n",
    "        except:\n",
    "            pass\n",
    "    return bw_iod_track_filename\n",
    "\n",
    "def hpr_to_bedgraph(hpr_idxs,\n",
    "                    coordinates_bin,\n",
    "                    tracks_directory,\n",
    "                    output_directory):\n",
    "\n",
    "    bedgraph_hpr_filename = os.path.join(tracks_directory,\n",
    "                                         'SELECTED_VARIABILITY_HOTSPOT.bedgraph')\n",
    "\n",
    "    bed_hpr_filename = os.path.join(output_directory,\n",
    "                                    'SELECTED_VARIABILITY_HOTSPOT.bed')\n",
    "\n",
    "    to_write = coordinates_bin.loc[hpr_idxs[hpr_idxs].index]\n",
    "    to_write.dropna(inplace=True)\n",
    "    to_write['bpstart'] = to_write['bpstart'].astype(int)\n",
    "    to_write['bpend'] = to_write['bpend'].astype(int)\n",
    "\n",
    "    to_write.to_csv(bedgraph_hpr_filename,\n",
    "                    sep='\\t',\n",
    "                    header=False,\n",
    "                    index=False)\n",
    "\n",
    "    if not os.path.exists(bed_hpr_filename) or recompute_all:\n",
    "\n",
    "        info('Writing the HPRs in: \"%s\"' % bed_hpr_filename)\n",
    "        sb.call('sort -k1,1 -k2,2n \"%s\" |'\n",
    "                ' bedtools merge -i stdin >  \"%s\"' % (bedgraph_hpr_filename,\n",
    "                                                      bed_hpr_filename),\n",
    "                shell=True)\n",
    "\n",
    "    return bed_hpr_filename\n",
    "\n",
    "def write_specific_regions(coordinates_bin,\n",
    "                           df_chip_hpr_zscore,\n",
    "                           specific_regions_directory,\n",
    "                           depleted,\n",
    "                           z_score_low,\n",
    "                           z_score_high):\n",
    "    # write target\n",
    "    info('Writing Specific Regions for each cell line...')\n",
    "    coord_zscore = coordinates_bin.copy()\n",
    "    for col in df_chip_hpr_zscore:\n",
    "        regions_specific_filename = 'Regions_specific_for_%s_z_%.2f.bedgraph' % (\n",
    "            os.path.basename(col).replace('.rpm', ''), z_score_high)\n",
    "        specific_output_filename = os.path.join(specific_regions_directory,\n",
    "                                                regions_specific_filename)\n",
    "        specific_output_bed_filename = specific_output_filename.replace('.bedgraph', '.bed')\n",
    "\n",
    "        if not os.path.exists(specific_output_bed_filename) or recompute_all:\n",
    "            if depleted:\n",
    "                z_score_high = -z_score_high\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] < z_score_high, col]\n",
    "            else:\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] > z_score_high, col]\n",
    "            coord_zscore.dropna().to_csv(specific_output_filename,\n",
    "                                         sep='\\t',\n",
    "                                         header=False,\n",
    "                                         index=False)\n",
    "\n",
    "            info('Writing:%s' % specific_output_bed_filename)\n",
    "            sb.call('sort -k1,1 -k2,2n \"%s\" |'\n",
    "                    ' bedtools merge -i stdin >  \"%s\"' % (specific_output_filename,\n",
    "                                                          specific_output_bed_filename),\n",
    "                    shell=True)\n",
    "    # write background\n",
    "    info('Writing Background Regions for each cell line...')\n",
    "    coord_zscore = coordinates_bin.copy()\n",
    "    for col in df_chip_hpr_zscore:\n",
    "\n",
    "        bg_output_filename = os.path.join(specific_regions_directory,\n",
    "                                          'Background_for_%s_z_%.2f.bedgraph' % (\n",
    "                                              os.path.basename(col).replace('.rpm', ''),\n",
    "                                              z_score_low))\n",
    "\n",
    "        bg_output_bed_filename = bg_output_filename.replace('.bedgraph', '.bed')\n",
    "\n",
    "        if not os.path.exists(bg_output_bed_filename) or recompute_all:\n",
    "\n",
    "            if depleted:\n",
    "                z_score_low = -z_score_low\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] > z_score_low, col]\n",
    "            else:\n",
    "                coord_zscore['z-score'] = df_chip_hpr_zscore.loc[\n",
    "                    df_chip_hpr_zscore.loc[:, col] < z_score_low, col]\n",
    "\n",
    "            coord_zscore.dropna().to_csv(bg_output_filename,\n",
    "                                         sep='\\t',\n",
    "                                         header=False,\n",
    "                                         index=False)\n",
    "\n",
    "            info('Writing:%s' % bg_output_bed_filename)\n",
    "            sb.call(\n",
    "                'sort -k1,1 -k2,2n -i \"%s\" |'\n",
    "                ' bedtools merge -i stdin >  \"%s\"' % (bg_output_filename,\n",
    "                                                      bg_output_bed_filename),\n",
    "                shell=True)\n",
    "\n",
    "\n",
    "def create_igv_track_file(hpr_iod_scores,\n",
    "                          bed_hpr_filename,\n",
    "                          bw_iod_track_filename,\n",
    "                          genome_name,\n",
    "                          output_directory,\n",
    "                          binned_sample_names,\n",
    "                          sample_names,\n",
    "                          disable_quantile_normalization):\n",
    "\n",
    "    import xml.etree.cElementTree as ET\n",
    "    import glob\n",
    "\n",
    "    igv_session_filename = os.path.join(output_directory, 'OPEN_ME_WITH_IGV.xml')\n",
    "    info('Creating an IGV session file (.xml) in: %s' % igv_session_filename)\n",
    "\n",
    "    session = ET.Element(\"Session\")\n",
    "    session.set(\"genome\", genome_name)\n",
    "    session.set(\"hasGeneTrack\", \"true\")\n",
    "    session.set(\"version\", \"7\")\n",
    "    resources = ET.SubElement(session, \"Resources\")\n",
    "    panel = ET.SubElement(session, \"Panel\")\n",
    "\n",
    "    resource_items = []\n",
    "    track_items = []\n",
    "\n",
    "    min_h = np.mean(hpr_iod_scores) - 2 * np.std(hpr_iod_scores)\n",
    "    max_h = np.mean(hpr_iod_scores) + 2 * np.std(hpr_iod_scores)\n",
    "    mid_h = np.mean(hpr_iod_scores)\n",
    "    # write the tracks\n",
    "    for binned_sample_name in binned_sample_names:\n",
    "        if disable_quantile_normalization:\n",
    "            track_full_path = os.path.join(output_directory, 'TRACKS', '%s.bw' % binned_sample_name)\n",
    "        else:\n",
    "            track_full_path = os.path.join(output_directory, 'TRACKS',\n",
    "                                           '%s_quantile_normalized.bw' % binned_sample_name)\n",
    "\n",
    "        track_filename = rem_base_path(track_full_path, output_directory)\n",
    "\n",
    "        if os.path.exists(track_full_path):\n",
    "            resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "            resource_items[-1].set(\"path\", track_filename)\n",
    "            track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "            track_items[-1].set('color', \"0,0,178\")\n",
    "            track_items[-1].set('id', track_filename)\n",
    "            track_items[-1].set(\"name\", binned_sample_name)\n",
    "\n",
    "    resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "    resource_items[-1].set(\"path\", rem_base_path(bw_iod_track_filename, output_directory))\n",
    "\n",
    "    track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "    track_items[-1].set('color', \"178,0,0\")\n",
    "    track_items[-1].set('id', rem_base_path(bw_iod_track_filename, output_directory))\n",
    "    track_items[-1].set('renderer', \"HEATMAP\")\n",
    "    track_items[-1].set(\"colorScale\",\n",
    "                        \"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (\n",
    "                            mid_h, min_h, mid_h, max_h))\n",
    "    track_items[-1].set(\"name\", 'VARIABILITY')\n",
    "\n",
    "    resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "    resource_items[-1].set(\"path\", rem_base_path(bed_hpr_filename, output_directory))\n",
    "    track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "    track_items[-1].set('color', \"178,0,0\")\n",
    "    track_items[-1].set('id', rem_base_path(bed_hpr_filename, output_directory))\n",
    "    track_items[-1].set('renderer', \"HEATMAP\")\n",
    "    track_items[-1].set(\"colorScale\",\n",
    "                        \"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (\n",
    "                            mid_h, min_h, mid_h, max_h))\n",
    "    track_items[-1].set(\"name\", 'HOTSPOTS')\n",
    "\n",
    "    for sample_name in sample_names:\n",
    "        track_full_path = \\\n",
    "            glob.glob(os.path.join(output_directory, 'SPECIFIC_REGIONS',\n",
    "                                   'Regions_specific_for_%s*.bedgraph' % sample_name))[0]\n",
    "        specific_track_filename = rem_base_path(track_full_path, output_directory)\n",
    "        if os.path.exists(track_full_path):\n",
    "            resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "            resource_items[-1].set(\"path\", specific_track_filename)\n",
    "\n",
    "            track_items.append(ET.SubElement(panel, \"Track\"))\n",
    "            track_items[-1].set('color', \"178,0,0\")\n",
    "            track_items[-1].set('id', specific_track_filename)\n",
    "            track_items[-1].set('renderer', \"HEATMAP\")\n",
    "            track_items[-1].set(\"colorScale\",\n",
    "                                \"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (\n",
    "                                    mid_h, min_h, mid_h, max_h))\n",
    "            track_items[-1].set(\"name\", 'REGION SPECIFIC FOR %s' % sample_name)\n",
    "\n",
    "    tree = ET.ElementTree(session)\n",
    "    tree.write(igv_session_filename, xml_declaration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[H A Y S T A C K   H O T S P O T]\n",
      "\n",
      "-SELECTION OF VARIABLE REGIONS- [Luca Pinello - lpinello@jimmy.harvard.edu]\n",
      "\n",
      "Version 0.4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print '\\n[H A Y S T A C K   H O T S P O T]'\n",
    "print('\\n-SELECTION OF VARIABLE REGIONS- [Luca Pinello - lpinello@jimmy.harvard.edu]\\n')\n",
    "print 'Version %s\\n' % HAYSTACK_VERSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Intialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:06:46:\n",
      "\t {'input_is_bigwig': False, 'th_rpm': 99, 'samples_filename': '/mnt/hd2/test_data/samples_names.txt', 'name': '', 'max_regions_percentage': 0.1, 'disable_quantile_normalization': False, 'z_score_low': 0.25, 'output_directory': '/mnt/hd2/test_data/OUTPUT8', 'blacklist': False, 'bin_size': 200, 'n_processes': 8, 'depleted': False, 'read_ext': 200, 'genome_name': 'hg19', 'z_score_high': 1.5, 'chrom_exclude': '', 'transformation': 'angle', 'recompute_all': False} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1.1\n",
    "parser = get_args()\n",
    "\n",
    "input_args=['/mnt/hd2/test_data/samples_names.txt',\n",
    "            'hg19',\n",
    "            '--output_directory',\n",
    "            '/mnt/hd2/test_data/OUTPUT8',\n",
    "            '--bin_size',\n",
    "            '200',\n",
    "             '--chrom_exclude',\n",
    "             '']\n",
    "\n",
    "args = parser.parse_args(input_args)\n",
    "info(vars(args))\n",
    "\n",
    "global recompute_all\n",
    "recompute_all = args.recompute_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1.2\n",
    "check_required_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1.3: create directories\n",
    "if args.name:\n",
    "    directory_name = 'HAYSTACK_HOTSPOTS_on_%s' % args.name\n",
    "else:\n",
    "    directory_name = 'HAYSTACK_HOTSPOTS'\n",
    "if args.output_directory:\n",
    "    output_directory = os.path.join(args.output_directory,\n",
    "                                    directory_name)\n",
    "else:\n",
    "    output_directory = directory_name\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "tracks_directory = os.path.join(output_directory,\n",
    "                                'TRACKS')\n",
    "if not os.path.exists(tracks_directory):\n",
    "    os.makedirs(tracks_directory)\n",
    "intermediate_directory = os.path.join(output_directory,\n",
    "                                      'INTERMEDIATE')\n",
    "if not os.path.exists(intermediate_directory):\n",
    "    os.makedirs(intermediate_directory)\n",
    "specific_regions_directory = os.path.join(output_directory,\n",
    "                                          'SPECIFIC_REGIONS')\n",
    "if not os.path.exists(specific_regions_directory):\n",
    "    os.makedirs(specific_regions_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:06:50:\n",
      "\t Initializing Genome:hg19 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1.4: get genome data\n",
    "chr_len_filename = initialize_genome(args.genome_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:07:03:\n",
      "\t Checking samples files location... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1.5: get filepaths\n",
    "sample_names, data_filenames = get_data_filepaths(args.samples_filename)\n",
    "binned_sample_names = ['%s.%dbp' % (sample_name, args.bin_size) for sample_name in sample_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create tiled genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 13:37:47:\n",
      "\t Creating bins of 200bp in /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/hg19.200bp.bins.sorted.bed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "genome_sorted_bins_file = create_tiled_genome(args.genome_name,\n",
    "                                              output_directory,\n",
    "                                              chr_len_filename,\n",
    "                                              args.bin_size,\n",
    "                                              args.chrom_exclude,\n",
    "                                              args.blacklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert files to genome-wide rpm tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 13:38:50:\n",
      "\t Processing:/mnt/hd2/test_data/K562H3k27ac_sorted_rmdup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:38:50:\n",
      "\t Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:39:13:\n",
      "\t Removing  optical duplicates \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:40:16:\n",
      "\t Processing:/mnt/hd2/test_data/Gm12878H3k27ac_sorted_rmdup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:40:16:\n",
      "\t Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:40:50:\n",
      "\t Removing  optical duplicates \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:42:31:\n",
      "\t Processing:/mnt/hd2/test_data/Hepg2H3k27ac_sorted_rmdup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:42:31:\n",
      "\t Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:42:52:\n",
      "\t Removing  optical duplicates \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:43:44:\n",
      "\t Processing:/mnt/hd2/test_data/H1hescH3k27ac_sorted_rmdup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:43:44:\n",
      "\t Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:44:32:\n",
      "\t Removing  optical duplicates \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:46:40:\n",
      "\t Processing:/mnt/hd2/test_data/HsmmH3k27ac_sorted_rmdup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:46:40:\n",
      "\t Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:47:16:\n",
      "\t Removing  optical duplicates \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:48:57:\n",
      "\t Processing:/mnt/hd2/test_data/NhlfH3k27ac_sorted_rmdup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:48:57:\n",
      "\t Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:49:21:\n",
      "\t Removing  optical duplicates \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:50:30:\n",
      "\t Processing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/FILTERED_BAMS/K562H3k27ac_sorted_rmdup.filtered.nodup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:50:30:\n",
      "\t Computing Scaling Factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:50:54:\n",
      "\t Scaling Factor: 5.110605e-02 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:50:54:\n",
      "\t Converting bam to bed and extending read length... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 13:59:24:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:00:35:\n",
      "\t Computing coverage over bins... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:00:35:\n",
      "\t Normalizing counts by scaling factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:00:35:\n",
      "\t Processing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/FILTERED_BAMS/Gm12878H3k27ac_sorted_rmdup.filtered.nodup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:00:35:\n",
      "\t Computing Scaling Factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:00:57:\n",
      "\t Scaling Factor: 5.763438e-02 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:00:57:\n",
      "\t Converting bam to bed and extending read length... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:04:59:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:05:59:\n",
      "\t Computing coverage over bins... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:05:59:\n",
      "\t Normalizing counts by scaling factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:05:59:\n",
      "\t Processing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/FILTERED_BAMS/Hepg2H3k27ac_sorted_rmdup.filtered.nodup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:05:59:\n",
      "\t Computing Scaling Factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:06:15:\n",
      "\t Scaling Factor: 7.806248e-02 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:06:15:\n",
      "\t Converting bam to bed and extending read length... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:08:39:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:09:24:\n",
      "\t Computing coverage over bins... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:09:24:\n",
      "\t Normalizing counts by scaling factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:09:24:\n",
      "\t Processing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/FILTERED_BAMS/H1hescH3k27ac_sorted_rmdup.filtered.nodup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:09:24:\n",
      "\t Computing Scaling Factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:10:01:\n",
      "\t Scaling Factor: 3.369318e-02 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:10:01:\n",
      "\t Converting bam to bed and extending read length... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:26:44:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:28:42:\n",
      "\t Computing coverage over bins... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:28:42:\n",
      "\t Normalizing counts by scaling factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:28:42:\n",
      "\t Processing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/FILTERED_BAMS/HsmmH3k27ac_sorted_rmdup.filtered.nodup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:28:42:\n",
      "\t Computing Scaling Factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:29:11:\n",
      "\t Scaling Factor: 4.428153e-02 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:29:11:\n",
      "\t Converting bam to bed and extending read length... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:33:00:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:34:22:\n",
      "\t Computing coverage over bins... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:34:22:\n",
      "\t Normalizing counts by scaling factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:34:22:\n",
      "\t Processing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/FILTERED_BAMS/NhlfH3k27ac_sorted_rmdup.filtered.nodup.bam \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:34:22:\n",
      "\t Computing Scaling Factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:34:41:\n",
      "\t Scaling Factor: 6.278750e-02 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:34:41:\n",
      "\t Converting bam to bed and extending read length... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:37:28:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:38:25:\n",
      "\t Computing coverage over bins... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:38:25:\n",
      "\t Normalizing counts by scaling factor... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:38:25:\n",
      "\t Making constant binned rpm values file: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/K562.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:39:17:\n",
      "\t Binned Bedgraph saved... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:39:17:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:39:45:\n",
      "\t Making constant binned rpm values file: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/GM12878.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:40:32:\n",
      "\t Binned Bedgraph saved... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:40:32:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:41:02:\n",
      "\t Making constant binned rpm values file: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/HEPG2.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:41:43:\n",
      "\t Binned Bedgraph saved... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:41:43:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:42:12:\n",
      "\t Making constant binned rpm values file: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/H1hesc.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:43:20:\n",
      "\t Binned Bedgraph saved... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:43:20:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:43:45:\n",
      "\t Making constant binned rpm values file: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/HSMM.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:44:37:\n",
      "\t Binned Bedgraph saved... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:44:37:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:45:05:\n",
      "\t Making constant binned rpm values file: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/NHLF.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:45:53:\n",
      "\t Binned Bedgraph saved... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 14:45:53:\n",
      "\t Converting BedGraph to BigWig \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.input_is_bigwig:\n",
    "    bigwig_filenames = copy_bigwigs(data_filenames,\n",
    "                                    sample_names,\n",
    "                                    tracks_directory)\n",
    "    binned_rpm_filenames = to_binned_tracks_if_bigwigs(bigwig_filenames,\n",
    "                                                       intermediate_directory,\n",
    "                                                       binned_sample_names,\n",
    "                                                       genome_sorted_bins_file)\n",
    "else:\n",
    "    bam_filtered_nodup_filenames = to_filtered_deduped_bams(data_filenames,\n",
    "                                                            output_directory,\n",
    "                                                            args.n_processes)\n",
    "\n",
    "    bedgraph_filenames, bigwig_filenames = to_normalized_extended_reads_tracks(bam_filtered_nodup_filenames,\n",
    "                                                                               sample_names,\n",
    "                                                                               tracks_directory,\n",
    "                                                                               chr_len_filename,\n",
    "                                                                               args.read_ext)\n",
    "    binned_rpm_filenames = to_binned_tracks(bedgraph_filenames,\n",
    "                                            binned_sample_names,\n",
    "                                            tracks_directory,\n",
    "                                            intermediate_directory,\n",
    "                                            chr_len_filename,\n",
    "                                            genome_sorted_bins_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create dataframe and perform quantile normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:07:50:\n",
      "\t Loading the processed tracks \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:07:51:\n",
      "\t Loading K562.200bp from file /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/K562.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:07:53:\n",
      "\t Loading GM12878.200bp from file /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/GM12878.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:07:54:\n",
      "\t Loading HEPG2.200bp from file /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/HEPG2.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:07:55:\n",
      "\t Loading H1hesc.200bp from file /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/H1hesc.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:07:57:\n",
      "\t Loading HSMM.200bp from file /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/HSMM.200bp.rpm \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:07:58:\n",
      "\t Loading NHLF.200bp from file /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/INTERMEDIATE/NHLF.200bp.rpm \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chip = load_binned_rpm_tracks(binned_sample_names, binned_rpm_filenames)\n",
    "coordinates_bin = pd.read_csv(genome_sorted_bins_file,\n",
    "                              names=['chr_id', 'bpstart', 'bpend'],\n",
    "                              sep='\\t',\n",
    "                              header=None,\n",
    "                              usecols=[0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GM12878.200bp</th>\n",
       "      <th>H1hesc.200bp</th>\n",
       "      <th>HEPG2.200bp</th>\n",
       "      <th>HSMM.200bp</th>\n",
       "      <th>K562.200bp</th>\n",
       "      <th>NHLF.200bp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.057634</td>\n",
       "      <td>0.158358</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.057634</td>\n",
       "      <td>0.084233</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076659</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051106</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139586</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112311</td>\n",
       "      <td>0.104083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051106</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185312</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068141</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>0.057634</td>\n",
       "      <td>0.147732</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076659</td>\n",
       "      <td>0.083717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>0.057634</td>\n",
       "      <td>0.230237</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110730</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127285</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       GM12878.200bp  H1hesc.200bp  HEPG2.200bp  HSMM.200bp  K562.200bp  \\\n",
       "10000       0.057634      0.158358     0.078063         0.0    0.144800   \n",
       "10001       0.057634      0.084233     0.078063         0.0    0.076659   \n",
       "10002       0.000000      0.073002     0.000000         0.0    0.051106   \n",
       "10003       0.000000      0.139586     0.078063         0.0    0.000000   \n",
       "10004       0.000000      0.112311     0.104083         0.0    0.051106   \n",
       "10005       0.000000      0.185312     0.078063         0.0    0.068141   \n",
       "10006       0.057634      0.147732     0.078063         0.0    0.076659   \n",
       "10007       0.057634      0.230237     0.078063         0.0    0.110730   \n",
       "10008       0.000000      0.210582     0.000000         0.0    0.144800   \n",
       "10009       0.000000      0.127285     0.078063         0.0    0.076659   \n",
       "\n",
       "       NHLF.200bp  \n",
       "10000    0.062787  \n",
       "10001    0.062787  \n",
       "10002    0.062787  \n",
       "10003    0.062787  \n",
       "10004    0.062787  \n",
       "10005    0.062787  \n",
       "10006    0.083717  \n",
       "10007    0.062787  \n",
       "10008    0.000000  \n",
       "10009    0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chip[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr_id</th>\n",
       "      <th>bpstart</th>\n",
       "      <th>bpend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>400</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chr_id  bpstart  bpend\n",
       "0   chr1        0    200\n",
       "1   chr1      200    400\n",
       "2   chr1      400    600\n",
       "3   chr1      600    800\n",
       "4   chr1      800   1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:09:35:\n",
      "\t Normalizing the data... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:09:43:\n",
      "\t Writing binned track: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/K562.200bp_quantile_normalized.bw \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:10:27:\n",
      "\t Writing binned track: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/GM12878.200bp_quantile_normalized.bw \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:11:13:\n",
      "\t Writing binned track: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/HEPG2.200bp_quantile_normalized.bw \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:11:58:\n",
      "\t Writing binned track: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/H1hesc.200bp_quantile_normalized.bw \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:12:39:\n",
      "\t Writing binned track: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/HSMM.200bp_quantile_normalized.bw \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:13:24:\n",
      "\t Writing binned track: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/NHLF.200bp_quantile_normalized.bw \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not args.disable_quantile_normalization:\n",
    "    info('Normalizing the data...')\n",
    "\n",
    "    df_chip, bigwig_binned_normalized_filenames = to_binned_normalized_tracks(df_chip,\n",
    "                                                                              coordinates_bin,\n",
    "                                                                              binned_sample_names,\n",
    "                                                                              chr_len_filename,\n",
    "                                                                              tracks_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GM12878.200bp</th>\n",
       "      <th>H1hesc.200bp</th>\n",
       "      <th>HEPG2.200bp</th>\n",
       "      <th>HSMM.200bp</th>\n",
       "      <th>K562.200bp</th>\n",
       "      <th>NHLF.200bp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.047199</td>\n",
       "      <td>0.125794</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.144631</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.047199</td>\n",
       "      <td>0.068317</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.073774</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.061146</td>\n",
       "      <td>0.046076</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.046076</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.104681</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.015804</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.084080</td>\n",
       "      <td>0.101087</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.046076</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.162404</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.071232</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>0.047199</td>\n",
       "      <td>0.117049</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.073774</td>\n",
       "      <td>0.077262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>0.047199</td>\n",
       "      <td>0.295187</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.116034</td>\n",
       "      <td>0.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.224018</td>\n",
       "      <td>0.046076</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.144631</td>\n",
       "      <td>0.024321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.096126</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.073774</td>\n",
       "      <td>0.024321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       GM12878.200bp  H1hesc.200bp  HEPG2.200bp  HSMM.200bp  K562.200bp  \\\n",
       "10000       0.047199      0.125794     0.063018    0.007487    0.144631   \n",
       "10001       0.047199      0.068317     0.063018    0.007487    0.073774   \n",
       "10002       0.024321      0.061146     0.046076    0.007487    0.046076   \n",
       "10003       0.024321      0.104681     0.063018    0.007487    0.015804   \n",
       "10004       0.024321      0.084080     0.101087    0.007487    0.046076   \n",
       "10005       0.024321      0.162404     0.063018    0.007487    0.071232   \n",
       "10006       0.047199      0.117049     0.063018    0.007487    0.073774   \n",
       "10007       0.047199      0.295187     0.063018    0.007487    0.116034   \n",
       "10008       0.024321      0.224018     0.046076    0.007487    0.144631   \n",
       "10009       0.024321      0.096126     0.063018    0.007487    0.073774   \n",
       "\n",
       "       NHLF.200bp  \n",
       "10000    0.047199  \n",
       "10001    0.047199  \n",
       "10002    0.047199  \n",
       "10003    0.047199  \n",
       "10004    0.047199  \n",
       "10005    0.047199  \n",
       "10006    0.077262  \n",
       "10007    0.047199  \n",
       "10008    0.024321  \n",
       "10009    0.024321  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    df_chip[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Determine HP regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:14:10:\n",
      "\t Determine HP regions \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:14:11:\n",
      "\t Estimated th_rpm:1.07407338979 \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:14:19:\n",
      "\t Using angle transformation \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:14:22:\n",
      "\t Selected 9.798753% regions (55127) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info('Determine HP regions')\n",
    "hpr_idxs, coordinates_bin, df_chip_hpr_zscore, hpr_iod_scores =\\\n",
    "    find_hpr_coordinates(df_chip,\n",
    "                         coordinates_bin,\n",
    "                         args.th_rpm,\n",
    "                         args.transformation,\n",
    "                         args.max_regions_percentage,\n",
    "                         output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GM12878.200bp</th>\n",
       "      <th>H1hesc.200bp</th>\n",
       "      <th>HEPG2.200bp</th>\n",
       "      <th>HSMM.200bp</th>\n",
       "      <th>K562.200bp</th>\n",
       "      <th>NHLF.200bp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.471543</td>\n",
       "      <td>-0.554750</td>\n",
       "      <td>2.224989</td>\n",
       "      <td>-0.475194</td>\n",
       "      <td>-0.235682</td>\n",
       "      <td>-0.487819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.585265</td>\n",
       "      <td>-0.511649</td>\n",
       "      <td>2.222103</td>\n",
       "      <td>-0.481676</td>\n",
       "      <td>-0.224615</td>\n",
       "      <td>-0.418898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.528274</td>\n",
       "      <td>-0.544893</td>\n",
       "      <td>2.228450</td>\n",
       "      <td>-0.446306</td>\n",
       "      <td>-0.293187</td>\n",
       "      <td>-0.415789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-0.728694</td>\n",
       "      <td>0.168171</td>\n",
       "      <td>-0.607663</td>\n",
       "      <td>-0.601485</td>\n",
       "      <td>2.138593</td>\n",
       "      <td>-0.368923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.751695</td>\n",
       "      <td>-0.594184</td>\n",
       "      <td>-0.419837</td>\n",
       "      <td>-0.660592</td>\n",
       "      <td>2.086587</td>\n",
       "      <td>0.339721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GM12878.200bp  H1hesc.200bp  HEPG2.200bp  HSMM.200bp  K562.200bp  \\\n",
       "100      -0.471543     -0.554750     2.224989   -0.475194   -0.235682   \n",
       "101      -0.585265     -0.511649     2.222103   -0.481676   -0.224615   \n",
       "102      -0.528274     -0.544893     2.228450   -0.446306   -0.293187   \n",
       "153      -0.728694      0.168171    -0.607663   -0.601485    2.138593   \n",
       "164      -0.751695     -0.594184    -0.419837   -0.660592    2.086587   \n",
       "\n",
       "     NHLF.200bp  \n",
       "100   -0.487819  \n",
       "101   -0.418898  \n",
       "102   -0.415789  \n",
       "153   -0.368923  \n",
       "164    0.339721  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chip_hpr_zscore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:16:34:\n",
      "\t hpr to bigwig \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:34:\n",
      "\t Generating variability track in bigwig format in:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/TRACKS/VARIABILITY.bw \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:36:\n",
      "\t hpr to bedgraph \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:36:\n",
      "\t Writing the HPRs in: \"/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SELECTED_VARIABILITY_HOTSPOT.bed\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info('hpr to bigwig')\n",
    "bw_iod_track_filename = hpr_to_bigwig(coordinates_bin,\n",
    "                                      tracks_directory,\n",
    "                                      chr_len_filename)\n",
    "info('hpr to bedgraph')\n",
    "bed_hpr_filename = hpr_to_bedgraph(hpr_idxs,\n",
    "                                   coordinates_bin,\n",
    "                                   tracks_directory,\n",
    "                                   output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Tue, 25 Jul 2017 15:16:39:\n",
      "\t Save files \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:39:\n",
      "\t Writing Specific Regions for each cell line... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Regions_specific_for_GM12878.200bp_z_1.50.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Regions_specific_for_H1hesc.200bp_z_1.50.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Regions_specific_for_HEPG2.200bp_z_1.50.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Regions_specific_for_HSMM.200bp_z_1.50.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Regions_specific_for_K562.200bp_z_1.50.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Regions_specific_for_NHLF.200bp_z_1.50.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:40:\n",
      "\t Writing Background Regions for each cell line... \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:41:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Background_for_GM12878.200bp_z_0.25.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:41:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Background_for_H1hesc.200bp_z_0.25.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:41:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Background_for_HEPG2.200bp_z_0.25.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:41:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Background_for_HSMM.200bp_z_0.25.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:42:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Background_for_K562.200bp_z_0.25.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:42:\n",
      "\t Writing:/mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/SPECIFIC_REGIONS/Background_for_NHLF.200bp_z_0.25.bed \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:42:\n",
      "\t Creating an IGV session file (.xml) in: /mnt/hd2/test_data/OUTPUT8/HAYSTACK_HOTSPOTS/OPEN_ME_WITH_IGV.xml \n",
      "\n",
      "INFO  @ Tue, 25 Jul 2017 15:16:42:\n",
      "\t All done! Ciao! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info('Save files')\n",
    "write_specific_regions(coordinates_bin,\n",
    "                       df_chip_hpr_zscore,\n",
    "                       specific_regions_directory,\n",
    "                       args.depleted,\n",
    "                       args.z_score_low,\n",
    "                       args.z_score_high)\n",
    "create_igv_track_file(hpr_iod_scores,\n",
    "                      bed_hpr_filename,\n",
    "                      bw_iod_track_filename,\n",
    "                      args.genome_name,\n",
    "                      output_directory,\n",
    "                      binned_sample_names,\n",
    "                      sample_names,\n",
    "                      args.disable_quantile_normalization)\n",
    "\n",
    "info('All done! Ciao!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
