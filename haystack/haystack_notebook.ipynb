{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "                                        HAYSTACK PIPELINE: SELECTION OF HOTSPOTS OF VARIABILITY AND ENRICHED MOTIFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import subprocess as sb\n",
    "import glob\n",
    "import shutil\n",
    "import multiprocessing\n",
    "\n",
    "try:\n",
    "    import cPickle as cp\n",
    "except:\n",
    "    import pickle as cp\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.use('Agg')\n",
    "import pylab as pl\n",
    "import xml.etree.cElementTree as ET\n",
    "from pybedtools import BedTool\n",
    "from bioutilities import Genome_2bit\n",
    "import pysam\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import  urllib2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAYSTACK_VERSION = \"0.4.0\"\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(levelname)-5s @ %(asctime)s:\\n\\t %(message)s \\n',\n",
    "                    datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                    stream=sys.stderr,\n",
    "                    filemode=\"w\", filename='example.log'\n",
    "                    )\n",
    "\n",
    "error = logging.critical\n",
    "warn = logging.warning\n",
    "debug = logging.debug\n",
    "info = logging.info\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commmon functions haystack hotspots\n",
    "\n",
    "def check_library(library_name):\n",
    "    try:\n",
    "        return __import__(library_name)\n",
    "    except:\n",
    "        error('You need to install %s module to use haystack!' % library_name)\n",
    "        sys.exit(1)\n",
    "\n",
    "def which(program):\n",
    "    import os\n",
    "    def is_exe(fpath):\n",
    "        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n",
    "\n",
    "    fpath, fname = os.path.split(program)\n",
    "    if fpath:\n",
    "        if is_exe(program):\n",
    "            return program\n",
    "    else:\n",
    "        for path in os.environ[\"PATH\"].split(os.pathsep):\n",
    "            path = path.strip('\"')\n",
    "            exe_file = os.path.join(path, program)\n",
    "            if is_exe(exe_file):\n",
    "                return exe_file\n",
    "    return None\n",
    "\n",
    "def check_program(binary_name, download_url=None):\n",
    "    if not which(binary_name):\n",
    "        error(\n",
    "            'You need to install and have the command #####%s##### in your PATH variable to use CRISPResso!\\n Please read the documentation!' % binary_name)\n",
    "        if download_url:\n",
    "            error('You can download it from here:%s' % download_url)\n",
    "        sys.exit(1)\n",
    "\n",
    "def check_file(filename):\n",
    "    try:\n",
    "        with open(filename):\n",
    "            pass\n",
    "    except IOError:\n",
    "        raise Exception('I cannot open the file: ' + filename)\n",
    "\n",
    "\n",
    "def quantile_normalization(A):\n",
    "    AA = np.zeros_like(A)\n",
    "    I = np.argsort(A, axis=0)\n",
    "    AA[I, np.arange(A.shape[1])] = np.mean(A[I, np.arange(A.shape[1])], axis=1)[:, np.newaxis]\n",
    "\n",
    "    return AA\n",
    "\n",
    "\n",
    "def smooth(x, window_len=200):\n",
    "    s = np.r_[x[window_len - 1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.hanning(window_len)\n",
    "    y = np.convolve(w / w.sum(), s, mode='valid')\n",
    "    return y[int(window_len / 2):-int(window_len / 2) + 1]\n",
    "\n",
    "\n",
    "# write the IGV session file\n",
    "def rem_base_path(path, base_path):\n",
    "    return path.replace(os.path.join(base_path, ''), '')\n",
    "\n",
    "\n",
    "def find_th_rpm(df_chip, th_rpm):\n",
    "    return np.min(df_chip.apply(lambda x: np.percentile(x, th_rpm)))\n",
    "\n",
    "\n",
    "def log2_transform(x):\n",
    "    return np.log2(x + 1)\n",
    "\n",
    "\n",
    "def angle_transform(x):\n",
    "    return np.arcsin(np.sqrt(x) / 1000000.0)\n",
    "\n",
    "\n",
    "def download_genome(name, output_directory=None):\n",
    "    urlpath = \"http://hgdownload.cse.ucsc.edu/goldenPath/%s/bigZips/%s.2bit\" % (name, name)\n",
    "    genome_url_origin = urllib2.urlopen(urlpath)\n",
    "\n",
    "    genome_filename = os.path.join(output_directory, \"%s.2bit\" % name)\n",
    "\n",
    "    print 'Downloading %s in %s...' % (urlpath, genome_filename)\n",
    "\n",
    "    if os.path.exists(genome_filename):\n",
    "        print 'File %s exists, skipping download' % genome_filename\n",
    "    else:\n",
    "\n",
    "        with open(genome_filename, 'wb') as genome_file_destination:\n",
    "            shutil.copyfileobj(genome_url_origin, genome_file_destination)\n",
    "\n",
    "        print 'Downloded %s in %s:' % (urlpath, genome_filename)\n",
    "\n",
    "    g = Genome_2bit(genome_filename, verbose=True)\n",
    "\n",
    "    chr_len_filename = os.path.join(output_directory, \"%s_chr_lengths.txt\" % name)\n",
    "    if not os.path.exists(chr_len_filename):\n",
    "        print 'Extracting chromosome lengths'\n",
    "        g.write_chr_len(chr_len_filename)\n",
    "        print 'Done!'\n",
    "    else:\n",
    "        print 'File %s exists, skipping generation' % chr_len_filename\n",
    "\n",
    "    meme_bg_filename = os.path.join(output_directory, \"%s_meme_bg\" % name)\n",
    "    if not os.path.exists(meme_bg_filename):\n",
    "        print 'Calculating nucleotide frequencies....'\n",
    "        g.write_meme_background(meme_bg_filename)\n",
    "        print 'Done!'\n",
    "    else:\n",
    "        print 'File %s exists, skipping generation' % meme_bg_filename\n",
    "        \n",
    "        \n",
    "def determine_path(folder):\n",
    "    _ROOT=os.getcwd()\n",
    "    return os.path.join(_ROOT,folder)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added functions\n",
    "\n",
    "def create_tiled_genome(genome_sorted_filtered_bins_file):\n",
    "\n",
    "    chr_len_filtered_filename = os.path.join(genome_directory, \"%s_chr_lengths_filtered.txt\" % genome_name)\n",
    "\n",
    "    genome_sorted_bins_file = os.path.join(output_directory,\n",
    "                                           '%s.%dbp.bins.sorted.bed' % (os.path.basename(genome_name),\n",
    "                                                                        bin_size))\n",
    "\n",
    "    with open(chr_len_filtered_filename, 'wb') as f:\n",
    "        f.writelines(line for line in open(chr_len_filename) if not re.search(chrom_exclude, line.split()[0]))\n",
    "\n",
    "    genome_windows = BedTool().\\\n",
    "        window_maker(g=chr_len_filtered_filename,w=bin_size).\\\n",
    "        sort().\\\n",
    "        saveas(genome_sorted_bins_file)\n",
    "\n",
    "    genome_windows.intersect(blacklist,\n",
    "                             wa=True,\n",
    "                             v=True,\n",
    "                             output=genome_sorted_filtered_bins_file)\n",
    "\n",
    "    try:\n",
    "        os.remove(genome_sorted_bins_file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def normalize_count(feature, scalar):\n",
    "    feature.name = str(int(feature.name) * scalar)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def get_scaling_factor(bam_filename):\n",
    "\n",
    "    infile = pysam.AlignmentFile(bam_filename, \"rb\")\n",
    "    numreads = infile.count(until_eof=True)\n",
    "    scaling_factor = (1.0 / float(numreads)) * 1000000\n",
    "\n",
    "    return scaling_factor\n",
    "\n",
    "def filter_dedup_bam(bam_filename, bam_filtered_nodup_filename):\n",
    "\n",
    "\n",
    "    bam_temp_filename=os.path.join(os.path.dirname(bam_filtered_nodup_filename),\n",
    "                                       '%s.temp%s' % os.path.splitext(os.path.basename(bam_filtered_nodup_filename)))\n",
    "\n",
    "    info('Removing  unmapped, mate unmapped, not primary alignment, low MAPQ reads, and reads failing qc')\n",
    "\n",
    "    cmd = 'sambamba view -f bam -l 0 -t %d -F \"not (unmapped or mate_is_unmapped or failed_quality_control or duplicate or secondary_alignment) and mapping_quality >= 30\" \"%s\"  -o \"%s\"' % (\n",
    "        n_processes, bam_filename, bam_temp_filename)\n",
    "    proc = sb.call(cmd, shell=True)\n",
    "\n",
    "    info('Removing  optical duplicates')\n",
    "\n",
    "    cmd = 'sambamba markdup  -l 5 -t %d --hash-table-size=17592186044416 --overflow-list-size=20000000 --io-buffer-size=256 \"%s\" \"%s\" ' % (\n",
    "        n_processes, bam_temp_filename, bam_filtered_nodup_filename)\n",
    "    proc = sb.call(cmd, shell=True)\n",
    "\n",
    "    try:\n",
    "        os.remove(bam_temp_filename)\n",
    "        os.remove(bam_temp_filename + '.bai')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def to_bedgraph(bam_filtered_nodup_filename, ext, rpm_filename):\n",
    "\n",
    "    info('Converting bam to bed and extending read length...')\n",
    "    bed_extended= BedTool(bam_filtered_nodup_filename). \\\n",
    "        bam_to_bed(). \\\n",
    "        slop(r=ext, l=0, s=True, g=chr_len_filename)\n",
    "\n",
    "    info('Computing Scaling Factor...')\n",
    "    scaling_factor = get_scaling_factor(bam_filtered_nodup_filename)\n",
    "\n",
    "    info('Computing coverage over bins...')\n",
    "    info('Normalizing counts by scaling factor...')\n",
    "\n",
    "    bedgraph = BedTool(genome_sorted_filtered_bins_file). \\\n",
    "        intersect(bed_extended, c=True). \\\n",
    "        each(normalize_count, scaling_factor).saveas(rpm_filename)\n",
    "\n",
    "    info('Bedgraph saved...')\n",
    "\n",
    "    return bedgraph\n",
    "\n",
    "def to_bigwig(bigwig_filename, rpm_filename):\n",
    "\n",
    "    if which('bedGraphToBigWig'):\n",
    "        if not os.path.exists(bigwig_filename) or recompute_all:\n",
    "            cmd = 'bedGraphToBigWig \"%s\" \"%s\" \"%s\"' % (rpm_filename, chr_len_filename, bigwig_filename)\n",
    "            proc = sb.call(cmd, shell=True)\n",
    "\n",
    "            try:\n",
    "                os.remove(rpm_filename)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    else:\n",
    "        info(\n",
    "            'Sorry I cannot create the bigwig file.\\nPlease download and install bedGraphToBigWig from here: http://hgdownload.cse.ucsc.edu/admin/exe/ and add to your PATH')\n",
    "\n",
    "def average_bigwig(bam_filename, binned_rpm_filename,bigwig_filename):\n",
    "\n",
    "    cmd = 'bigWigAverageOverBed %s %s  /dev/stdout | sort -s -n -k 1,1 | cut -f5 > %s' % (\n",
    "        bam_filename, genome_sorted_filtered_bins_file, binned_rpm_filename)\n",
    "    sb.call(cmd, shell=True)\n",
    "    shutil.copy2(bam_filename, bigwig_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'os' is not defined",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6df92334dc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msamples_filename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haystack_analysis/sample_names.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutput_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'haystack_analysis/output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmotif_directory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'motif_databases'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6df92334dc32>\u001b[0m in \u001b[0;36mdetermine_path\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_ROOT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msamples_filename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haystack_analysis/sample_names.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'os' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "samples_filename= determine_path('test_data/samples_names.txt')\n",
    "output_directory=determine_path( 'haystack_analysis/output')\n",
    "motif_directory= determine_path('motif_daatabases')\n",
    "annotation_directory=determine_path('gene_annotations')\n",
    "genome_directory=determine_path('genomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#mandatory\n",
    "parser = argparse.ArgumentParser(description='HAYSTACK Parameters')\n",
    "parser.add_argument('samples_filename_or_bam_folder', type=str,  help='A tab delimeted file with in each row (1) a sample name, (2) the path to the corresponding bam filename, (3 optional) the path to the corresponding gene expression filaneme. Alternatively it is possible to specify a folder containing some .bam files to analyze.')\n",
    "parser.add_argument('genome_name', type=str,  help='Genome assembly to use from UCSC (for example hg19, mm9, etc.)')\n",
    "\n",
    "#optional\n",
    "parser.add_argument('--name',  help='Define a custom output filename for the report', default='')\n",
    "parser.add_argument('--output_directory',type=str, help='Output directory (default: current directory)',default='')\n",
    "parser.add_argument('--bin_size', type=int,help='bin size to use(default: 500bp)',default=500)\n",
    "parser.add_argument('--recompute_all',help='Ignore any file previously precalculated fot the command haystack_hotstpot',action='store_true')\n",
    "parser.add_argument('--depleted', help='Look for cell type specific regions with depletion of signal instead of enrichment',action='store_true')\n",
    "parser.add_argument('--input_is_bigwig', help='Use the bigwig format instead of the bam format for the input. Note: The files must have extension .bw',action='store_true')\n",
    "parser.add_argument('--disable_quantile_normalization',help='Disable quantile normalization (default: False)',action='store_true')\n",
    "parser.add_argument('--transformation',type=str,help='Variance stabilizing transformation among: none, log2, angle (default: angle)',default='angle',choices=['angle', 'log2', 'none'])\n",
    "parser.add_argument('--z_score_high', type=float,help='z-score value to select the specific regions(default: 1.5)',default=1.5)\n",
    "parser.add_argument('--z_score_low', type=float,help='z-score value to select the not specific regions(default: 0.25)',default=0.25)\n",
    "parser.add_argument('--th_rpm',type=float,help='Percentile on the signal intensity to consider for the hotspots (default: 99)', default=99)\n",
    "parser.add_argument('--meme_motifs_filename', type=str, help='Motifs database in MEME format (default JASPAR CORE 2016)')\n",
    "parser.add_argument('--motif_mapping_filename', type=str, help='Custom motif to gene mapping file (the default is for JASPAR CORE 2016 database)')\n",
    "parser.add_argument('--plot_all',  help='Disable the filter on the TF activity and correlation (default z-score TF>0 and rho>0.3)',action='store_true')\n",
    "parser.add_argument('--n_processes',type=int, help='Specify the number of processes to use. The default is #cores available.',default=multiprocessing.cpu_count())\n",
    "parser.add_argument('--temp_directory',  help='Directory to store temporary files  (default: /tmp)', default='/tmp')\n",
    "parser.add_argument('--version',help='Print version and exit.',action='version', version='Version %s' % HAYSTACK_VERSION)\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "# HAYSTACK Parameters\n",
    "\n",
    "samples_filename_or_bam_folder = samples_filename\n",
    "genome_name = 'hg19'\n",
    "\n",
    "# optional\n",
    "name = ''\n",
    "output_directory = output_directory\n",
    "bin_size = 200\n",
    "recompute_all = True\n",
    "depleted = True\n",
    "input_is_bigwig = False\n",
    "disable_quantile_normalization = False\n",
    "transformation = 'angle'\n",
    "z_score_high = 1.5\n",
    "z_score_low = 0.25\n",
    "th_rpm = 99\n",
    "meme_motifs_filename = '' #os.path.join(MOTIF_DIR, 'JASPAR_CORE_2016_vertebrates.meme')\n",
    "motif_mapping_filename = '' #os.path.join(MOTIF_DIR, 'JASPAR_CORE_2016_vertebrates_mapped_to_gene_human_mouse.txt')\n",
    "plot_all = True\n",
    "use_X_Y= True\n",
    "chrom_exclude = '' #'_|chrM|chrX|chrY'\n",
    "ext=200\n",
    "n_processes = multiprocessing.cpu_count()\n",
    "temp_directory = '' #os.path.join(_ROOT, 'tmp')\n",
    "version = 'Version %s' % HAYSTACK_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if meme_motifs_filename:\n",
    "    check_file(meme_motifs_filename)\n",
    "\n",
    "if motif_mapping_filename:\n",
    "    check_file(motif_mapping_filename)\n",
    "\n",
    "# if not os.path.exists(temp_directory):\n",
    "#    error('The folder specified with --temp_directory: %s does not exist!' % temp_directory)\n",
    "#    sys.exit(1)\n",
    "\n",
    "if input_is_bigwig:\n",
    "    extension_to_check = '.bw'\n",
    "    info('Input is set BigWig (.bw)')\n",
    "else:\n",
    "    extension_to_check = '.bam'\n",
    "    info('Input is set compressed SAM (.bam)')\n",
    "\n",
    "if name:\n",
    "    directory_name = 'HAYSTACK_PIPELINE_RESULTS_on_%s' % name\n",
    "\n",
    "else:\n",
    "    directory_name = 'HAYSTACK_PIPELINE_RESULTS'\n",
    "\n",
    "if output_directory:\n",
    "    output_directory = os.path.join(output_directory, directory_name)\n",
    "else:\n",
    "    output_directory = directory_name\n",
    "\n",
    "# check folder or sample filename\n",
    "\n",
    "USE_GENE_EXPRESSION = True\n",
    "\n",
    "if os.path.isfile(samples_filename_or_bam_folder):\n",
    "    BAM_FOLDER = False\n",
    "    bam_filenames = []\n",
    "    gene_expression_filenames = []\n",
    "    sample_names = []\n",
    "\n",
    "    dir_path = os.path.dirname(os.path.realpath(samples_filename_or_bam_folder))\n",
    "\n",
    "    with open(samples_filename_or_bam_folder) as infile:\n",
    "        for line in infile:\n",
    "\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            if line.startswith('#'):  # skip optional header line or empty lines\n",
    "                info('Skipping header/comment line:%s' % line)\n",
    "                continue\n",
    "\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            n_fields = len(fields)\n",
    "\n",
    "            if n_fields == 2:\n",
    "\n",
    "                USE_GENE_EXPRESSION = False\n",
    "\n",
    "                sample_names.append(fields[0])\n",
    "                bam_filenames.append(fields[1])\n",
    "\n",
    "            elif n_fields == 3:\n",
    "\n",
    "                USE_GENE_EXPRESSION = USE_GENE_EXPRESSION and True\n",
    "\n",
    "                sample_names.append(fields[0])\n",
    "                bam_filenames.append(fields[1])\n",
    "                gene_expression_filenames.append(fields[2])\n",
    "            else:\n",
    "                error('The samples file format is wrong!')\n",
    "\n",
    "    bam_filenames = [os.path.join(dir_path, filename) for filename in bam_filenames]\n",
    "    gene_expression_filenames = [os.path.join(dir_path, filename) for filename in gene_expression_filenames]\n",
    "\n",
    "else:\n",
    "    if os.path.exists(samples_filename_or_bam_folder):\n",
    "        BAM_FOLDER = True\n",
    "        USE_GENE_EXPRESSION = False\n",
    "        bam_filenames = glob.glob(os.path.join(samples_filename_or_bam_folder, '*' + extension_to_check))\n",
    "\n",
    "        if not bam_filenames:\n",
    "            error('No bam/bigwig  files to analyze in %s. Exiting.' % samples_filename_or_bam_folder)\n",
    "            sys.exit(1)\n",
    "\n",
    "        sample_names = [os.path.basename(bam_filename).replace(extension_to_check, '') for bam_filename in\n",
    "                        bam_filenames]\n",
    "    else:\n",
    "        error(\"The file or folder %s doesn't exist. Exiting.\" % samples_filename_or_bam_folder)\n",
    "        sys.exit(1)\n",
    "\n",
    "# check all the files before starting\n",
    "info('Checking samples files location...')\n",
    "for bam_filename in bam_filenames:\n",
    "    check_file(bam_filename)\n",
    "\n",
    "if USE_GENE_EXPRESSION:\n",
    "    for gene_expression_filename in gene_expression_filenames:\n",
    "        check_file(gene_expression_filename)\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# copy back the file used\n",
    "if not BAM_FOLDER:\n",
    "    shutil.copy2(samples_filename_or_bam_folder, output_directory)\n",
    "\n",
    "# write hotspots conf files\n",
    "sample_names_hotspots_filename = os.path.join(output_directory, 'sample_names_hotspots.txt')\n",
    "\n",
    "with open(sample_names_hotspots_filename, 'w+') as outfile:\n",
    "    for sample_name, bam_filename in zip(sample_names, bam_filenames):\n",
    "        outfile.write('%s\\t%s\\n' % (sample_name, bam_filename))\n",
    "\n",
    "# write tf activity  conf files\n",
    "if USE_GENE_EXPRESSION:\n",
    "    sample_names_tf_activity_filename = os.path.join(output_directory, 'sample_names_tf_activity.txt')\n",
    "\n",
    "    with open(sample_names_tf_activity_filename, 'w+') as outfile:\n",
    "        for sample_name, gene_expression_filename in zip(sample_names, gene_expression_filenames):\n",
    "            outfile.write('%s\\t%s\\n' % (sample_name, gene_expression_filename))\n",
    "\n",
    "    tf_activity_directory = os.path.join(output_directory, 'HAYSTACK_TFs_ACTIVITY_PLANES')\n",
    "\n",
    "\n",
    "if name:\n",
    "    directory_name = 'HAYSTACK_HOTSPOTS_on_%s' % name\n",
    "\n",
    "else:\n",
    "    directory_name = 'HAYSTACK_HOTSPOTS'\n",
    "\n",
    "if output_directory:\n",
    "    output_directory = os.path.join(output_directory, directory_name)\n",
    "else:\n",
    "    output_directory = directory_name\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "tracks_directory = os.path.join(output_directory, 'TRACKS')\n",
    "if not os.path.exists(tracks_directory):\n",
    "    os.makedirs(tracks_directory)\n",
    "\n",
    "intermediate_directory = os.path.join(output_directory, 'INTERMEDIATE')\n",
    "if not os.path.exists(intermediate_directory):\n",
    "    os.makedirs(intermediate_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info('Initializing Genome:%s' % genome_name)\n",
    "\n",
    "genome_2bit = os.path.join(genome_directory, genome_name + '.2bit')\n",
    "\n",
    "if os.path.exists(genome_2bit):\n",
    "    genome = Genome_2bit(genome_2bit)\n",
    "else:\n",
    "    info(\"\\nIt seems you don't have the required genome file.\")\n",
    "\n",
    "    download_genome(genome_name, genome_directory)\n",
    "    if os.path.exists(genome_2bit):\n",
    "        info('Genome correctly downloaded!')\n",
    "        genome = Genome_2bit(genome_2bit)\n",
    "    else:\n",
    "        error('Sorry I cannot download the required file for you. Check your Internet connection.')\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chr_len_filename = os.path.join(genome_directory, \"%s_chr_lengths.txt\" % genome_name)\n",
    "blacklist = os.path.join(genome_directory,'blacklist.bed')\n",
    "check_file(chr_len_filename)\n",
    "check_file(blacklist)\n",
    "\n",
    "genome_sorted_filtered_bins_file = os.path.join(output_directory,\n",
    "                                                    '%s.%dbp.bins.sorted.filterd.bed' % (os.path.basename(genome_name),\n",
    "                                                                                         bin_size))\n",
    "\n",
    "if not os.path.exists(genome_sorted_filtered_bins_file) or recompute_all:\n",
    "    info('Creating bins of %dbp in %s' % (bin_size, genome_sorted_filtered_bins_file))\n",
    "    create_tiled_genome(genome_sorted_filtered_bins_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bam files to genome-wide rpm tracks\n",
    "for base_name, bam_filename in zip(sample_names, bam_filenames):\n",
    "\n",
    "    # bam_filename=bam_filenames[0]\n",
    "    # base_name=sample_names[0]\n",
    "\n",
    "    info('Processing:%s' % bam_filename)\n",
    "\n",
    "    rpm_filename = os.path.join(tracks_directory, '%s.bedgraph' % base_name)\n",
    "    binned_rpm_filename = os.path.join(intermediate_directory, '%s.%dbp.rpm' % (base_name, bin_size))\n",
    "    bigwig_filename = os.path.join(tracks_directory, '%s.bw' % base_name)\n",
    "    bam_filtered_nodup_filename = os.path.join(dir_path,\n",
    "                                               '%s.filtered.nodup%s' % (os.path.splitext(\n",
    "                                                   os.path.basename(bam_filename))))\n",
    "\n",
    "    if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "\n",
    "        if input_is_bigwig and which('bigWigAverageOverBed'):\n",
    "\n",
    "            average_bigwig(bam_filename, binned_rpm_filename, bigwig_filename)\n",
    "\n",
    "        else:\n",
    "            info('Filtering and deduping BAM file...')\n",
    "\n",
    "            filter_dedup_bam(bam_filename, bam_filtered_nodup_filename)\n",
    "\n",
    "            info('Building BedGraph RPM track...')\n",
    "\n",
    "            bedgraph = to_bedgraph(bam_filtered_nodup_filename, ext, rpm_filename)\n",
    "\n",
    "            info('Converting BedGraph to BigWig')\n",
    "\n",
    "            to_bigwig(bigwig_filename, rpm_filename)\n",
    "\n",
    "            info('Making constant binned (%dbp) rpm values file' % bin_size)\n",
    "\n",
    "            bedgraph.to_dataframe()['name'].to_csv(binned_rpm_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}